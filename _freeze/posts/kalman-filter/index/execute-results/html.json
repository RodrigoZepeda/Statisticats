{
  "hash": "8b1f9c09968088481792ec9ffe4f3391",
  "result": {
    "markdown": "---\ntitle: \"El filtro de Kalman 1 (introducción)\"\ndate: \"2022-11-12\"\ncategories: [state space models / modelos de espacios de estados, Kalman filter / filtro de Kalman, smoothing /suavizamiento, statistics / estadística, linear models / modelos lineales, mínimos cuadrados recursivos / recursive least squares, python]\nimage: \"filter.jpg\"\nlang: es\ndraft: true\nbibliography: references.bib\nabstract: \"En esta entrada discutimos la idea que inspiró al filtro de Kalman. Empezamos con mínimos cuadrados, pasamos a mínimos cuadrados recursivos y finalmente al planteamiento y solución del filtro de Kalman clásico.\"\ncrossref:\n  eq-prefix: la ecuación\nformat:\n  html:\n    code-fold: true\n---\n\n![Dmitry Makeev, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons](filter.jpg){fig-alt=\"Fotografía de un filtro polarizado para cámara fotográfica tipo reflex y el estuche del filtro en color blanco.\" fig-align=\"center\"}\n\n## Mínimos cuadrados\n\nEl problema clásico de regresión consiste en estimar $\\mathbf{\\beta} = (\\beta_1, \\beta_2, \\dots, \\beta_p)^{\\intercal}$ dadas $n$ observaciones $\\mathbf{x}_t = (x_{1,t}, \\dots, x_{p,t})^{\\intercal}$ ($t = 1, \\dots,n)$ y $\\mathbf{y}_t = (y_1, \\dots, y_t)^{\\intercal}$ las cuales están relacionadas de la siguiente forma:\n\n$$\ny_t = \\sum_{i = 1}^{p}  x_{i,t}\\beta_i + \\epsilon_t = \\mathbf{x}_t^{\\intercal} \\mathbf{\\beta} + \\epsilon_t\n$$ donde los términos de error $\\{\\epsilon_t\\}_{t=1}^n$ son variables aleatorias independientes con media cero y varianza $\\sigma^2 < \\infty$.\n\nUna de las formas clásicas de estimación es minimizando la suma de cuadrados de los errores de estimación dada por:\n\n$$\nS(\\mathbf{\\beta}) = \\sum_{t=1}^n \\big(y_t - \\mathbf{x}_t^{\\intercal}\\mathbf{\\beta}  \\big)^2\n$$ La cual puede escribirse de manera matricial como la minimización de la norma-2: $$\nS(\\beta) = (\\mathbf{y} - \\mathbf{X} \\mathbf{\\beta})^{\\intercal}(\\mathbf{y} - \\mathbf{X} \\mathbf{\\beta}) = \\|\\mathbf{y} - \\mathbf{X} \\mathbf{\\beta}\\|^2_2\n$$ {#eq-sumsq}\n\ncon $\\mathbf{X} = \\big(\\mathbf{x}_1|\\mathbf{x}_2|\\dots|\\mathbf{x}_n\\big)^{\\intercal}$ y $\\mathbf{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_n)^{\\intercal}$.\n\nSi la matriz $\\mathbf{X}$ es de rango completo, la solución al sistema es la proyección ortogonal:\n\n$$\n\\hat\\beta=(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}y = \\left(\\sum\\limits_{t=1}^n \\mathbf{x}_t \\mathbf{x}_t^{\\intercal} \\right)^{-1} \\sum_{t=1}^n\\mathbf{x}_t \\mathbf{y}_t\n$$\n\nY el resultado es la línea (hiperplano) que minimiza los errores:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\nsns.set(rc={'text.usetex' : True})\nsns.set_theme(style=\"white\")\n\n#Simulamos datos\nnp.random.seed(28802574)\nx       = np.linspace(0,10)\nepsilon = np.random.normal(loc=0.0, scale=2.5, size=len(x))\ny_true  = 3*x + 15\ny_obs   = y_true + epsilon\n\n#Agregamos una columna de 1's a las X para el intercepto\nx_extended = sm.add_constant(x)\n\n#Obtenemos modelo de regresión\nmodel = sm.OLS(y_obs, x_extended)\n\n#Ajustamos modelo\nresults = model.fit()\n\n#Predecimos\ny_pred = results.predict()\n\n#Gráfica\nf, ax = plt.subplots(figsize=(8, 4.5))\nf.suptitle('Estimación por mínimos cuadrados')\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(x=x, y=y_obs, color=\"#497174\")\nsns.lineplot(x=x, y=y_pred, color='#EB6440', alpha=0.5)\nax.set_title(r'$y = \\beta_0 + \\beta_1 x$')\nax.set_xlabel(r'$x$')\nax.set_ylabel(r'$y$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=661 height=440}\n:::\n:::\n\n\n## Mínimos cuadrados recursivos\n\nSi imaginamos un escenario donde los datos llegan de manera ordenada (como en una serie de tiempo) $\\mathbf{x}_1, \\mathbf{x}_2, \\dots$ podemos pensar en generar distintos estimadores de $\\beta$ dados por la colección $\\{\\hat{\\beta}_i\\}_{i=1}^n$ generados conforme se observan más $\\mathbf{x}$. Es decir, el primer estimador de $\\beta$, $\\hat{\\beta}_1$, se estimaría sólo con $y_1$ y $\\mathbf{x}_1$ mientras que un estimador al tiempo $t$, $\\hat{\\beta}_t$, se generaría con las primeras $t$ observaciones: $y_1, \\dots, y_t$ y $\\mathbf{x}_1, \\dots, \\mathbf{x}_t$.\n\nLas betas se pueden estimar recursivamente pues su diferencia es una matriz que sólo depende del término más reciente (ver @triantafyllopoulos2021bayesian para su obtención) como:\n\n$$\n\\hat\\beta_t = \\hat\\beta_{t-1} + K_t e_t\n$$\ndonde $K_t = \\Big(\\sum_{i=0}^{t-1} \\mathbf{x}_{t-i} \\mathbf{x}_{t-1}^{\\intercal}\\Big)^{-1} \\mathbf{x}_t$ y $e_t = (\\mathbf{y}_t - \\mathbf{x}_t^{\\intercal} \\hat{\\mathbf{\\beta}}_{t-1})$. \n\nPodemos generalizar ésta idea de estimación secuencial decidiendo agregar un factor de descuento $\\delta\\in(0,1]$. De esta manera para la suma de cuadrados *pesaría* más la información reciente:\n\n$$\nS(\\mathbf{\\beta}) = \\sum_{t=1}^n \\delta^{n-i}\\big(y_t - \\mathbf{x}_t^{\\intercal}\\mathbf{\\beta}  \\big)^2\n$$\n\nÉsta misma suma se puede reescribir como una minimización clásica de cuadrados considerando $\\tilde{\\mathbf{X}}_t= \\big(\\delta^{(t-1)/2}\\mathbf{x}_1|\\delta^{(t-2)/2}\\mathbf{x}_2|\\dots|\\delta^{0}\\mathbf{x}_t\\big)^{\\intercal}$ y $\\tilde{\\mathbf{y}}_t = (\\delta^{(t-1)/2}y_1, \\dots, \\delta^{0}y_t)^{\\intercal}$ en @eq-sumsq. El estimador $\\hat\\beta_t$ de $\\beta$ al momento $t$ está dado por:\n\n$$\n\\hat{\\beta}_t = (\\tilde{\\mathbf{X}_t}^{\\intercal}\\tilde{\\mathbf{X}_t})^{-1}\\tilde{\\mathbf{X}_t}^{\\intercal}\\tilde{y}_t\n$$ En el siguiente gráfico podemos ver cómo varía la estimación de $\\beta$ dada por $\\hat{\\beta}_t$ conforme avanza $t$:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#Obtenemos modelo de regresión\nmodel = sm.RecursiveLS(y_obs, x_extended)\n\n#Ajustamos modelo\nresults = model.fit()\n\n#Predecimos\ny_pred = results.predict()\n\n#Obtenemos los coeficientes\nbeta0 = results.recursive_coefficients.filtered[0]\nbeta1 = results.recursive_coefficients.filtered[1]\ntime  = range(0, len(x))\n\n#Gráfica\nf, ax = plt.subplots(1,3, figsize=(8,4.5))\nf.suptitle('Estimación por mínimos cuadrados recursivos')\nsns.despine(f, left=True, bottom=True)\nsns.lineplot(ax=ax[0], x=time, y=beta0, color='#497174', alpha=0.5)\nax[0].set_title(r'Intercepto ($\\beta_0$)')\nax[0].set_xlabel(r'$t$')\nax[0].set_ylabel(r'$\\beta_0$')\n\nsns.lineplot(ax=ax[1], x=time, y=beta1, color='#EB6440', alpha=0.5)\nax[1].set_title(r'Pendiente ($\\beta_1$)')\nax[1].set_xlabel(r'$t$')\nax[1].set_ylabel(r'$\\beta_1$')\n\nsns.scatterplot(ax=ax[2], x=x, y=y_obs, color=\"#497174\")\nsns.lineplot(ax=ax[2], x=x, y=y_pred, color='#EB6440', alpha=0.5)\nax[2].set_title(r'$y = \\beta_0 + \\beta_1 x$')\nax[2].set_xlabel(r'$x$')\nax[2].set_ylabel(r'$y$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=673 height=440}\n:::\n:::\n\n\n## El filtro de Kalman\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}