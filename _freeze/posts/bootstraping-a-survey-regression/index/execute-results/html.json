{
  "hash": "42dfd458c5f3ed5c1c055a4209bca0cf",
  "result": {
    "markdown": "---\ntitle: \"Bootstrapping a survey regression\"\ndate: \"2023-08-29\"\ncategories: [R, bootstrap, statistical-inference, survey analysis, survey]\nimage: \"boot.jpg\"\nlang: en\ndraft: false\nabstract: \"In this entry I discuss how to estimate regression coefficients using *bootstrapped* samples. This allows to calculate regressions even for cases where the `survey` function is not available.\"\nbibliography: references.bib\ncrossref:\n  eq-prefix: equation\nformat:\n  html:\n    fig-align: center\n    code-fold: false\n---\n\n\n## Bootstrap 101\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(survey, svrep, matrixStats, modelsummary)\n```\n:::\n\n\nThe main idea behind bootstrap is that resampling the random sample allows us to generate replicates of the sampling process itself. Using these re-sampled data, we can generate our desired estimators. These estimators will be asymptotically equivalent to the desired quantities.\n\n### Bootstraping a mean\n\nAs an example, consider a random sample of `n = 1000` samples from a $Normal(0,1)$ distribution:\n$$\n\\{X_1, X_2, \\dots, X_n\\} \\text{ with } X_i\\sim \\text{Normal}(0,1)\n$$\n\nIn `R` this can be obtained as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples <- rnorm(1000)\n```\n:::\n\n\nOne can estimate the mean with the classical estimator, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01726104\n```\n:::\n:::\n\n\nIn `R` this has already been programmed as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(samples)$conf.int\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.04433005  0.07885213\nattr(,\"conf.level\")\n[1] 0.95\n```\n:::\n:::\n\n\nThe bootstrap estimate relies on going over several samples and calculating the mean for each of the resamples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresample_means <- rep(NA, 1000) #Save the mean of the re-samples\nfor (i in 1:1000){\n  resample <- sample(samples, size = 1000, replace = TRUE)\n  resample_means[i] <- mean(resample)\n}\n\n#Bootstrap estimate:\nmean(resample_means)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01633088\n```\n:::\n:::\n\n\nThe **Wald-type** confidence intervals at level $(1-\\alpha)\\times 100\\%$ are given by:\n$$\n\\bar{X}\\pm t_{1-\\alpha/2}\\sqrt{\\widehat{\\textrm{Var}}_{\\text{Boot}}(\\bar{X})}\n$$\nwhere the unbiased estimator of the variance is:\n\n$$\n\\widehat{\\textrm{Var}}_{\\text{Boot}}(\\bar{X}) = \\sum\\limits_{i = 1}^{n} (\\bar{X}_{\\text{Boot},i} - \\bar{X})^2\n$$\n\nwhere $\\bar{X}_{\\text{Boot},i}$ is the estimate of the mean in the $i$-th bootstrap sample. \n\nIn `R`, these confidence intervals can be computed as:\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_estim  <- mean(resample_means)\nvar_estim <- var(resample_means)\nt_1_alpha <- qt(1 - 0.05/2, df = 1000)\nci_wald   <- c(\"Lower\" = mu_estim - t_1_alpha*sqrt(var_estim), \n               \"Upper\" = mu_estim + t_1_alpha*sqrt(var_estim))\n```\n:::\n\n\n> Notice that the formula is not divided by $n$ as we are estimating the standard error of the variance for the **mean** statistic and not of the $X_i$. Intuitively, it makes sense **not** to divide by the number of bootstrap samples. If we did we could then artificially decrease uncertainty around estimates just by taking more boostrap samples without gathering more data!\n\nAnother type of confidence intervals ([not recommended](https://stats.stackexchange.com/a/357498/81181)) is the **percentile confidence interval** given by the $\\alpha/2$ and $1 - \\alpha/2$ sample-percentiles of the bootstrap sample:\n\n$$\n[\\bar{X}_{\\text{Boot}, (\\alpha/2)}, \\bar{X}_{\\text{Boot}, (1 - \\alpha/2)}]\n$$\nIn `R`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Bootstrap estimate:\nquantile(resample_means, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2.5%       97.5% \n-0.04485072  0.07992000 \n```\n:::\n:::\n\n\nIn this particular case, they all are excellent estimates for our quantity of interest:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Generating bootstrap samples for surveys\n\nThere are already pre-programmed functions in the `survey` package that allow users to calculate some quantities of interest via bootstrap. In this example, we'll go through the first part of the [`survey` example vignette](https://cran.r-project.org/web/packages/survey/vignettes/survey.pdf) but with bootstrap. We'll load the data and setup the survey design, closely following the vignette:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survey)\ndata(api)\nclus1 <- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)\n```\n:::\n\n\nThen, we'll estimate the mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvymean(~api00, clus1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        mean     SE\napi00 644.17 23.542\n```\n:::\n:::\n\n\nTo change into bootstrap mode one has only to replicate the design\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_clus1 <- as_bootstrap_design(clus1, replicates = 1000)\nsvymean(~api00, boot_clus1) #Mean but now using 1000 re-samples\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        mean     SE\napi00 644.17 22.599\n```\n:::\n:::\n\n\nNotice that we had to use the design to generate the replicates. This is an important thing to keep in mind as for complex surveys one cannot simply resample without considering the design [@mashreghi2016survey](http://dx.doi.org/10.1214/16-SS113).\n\n## Playing with the bootstrapped sample\n\nNow, what happens if we actually need to obtain the replicates (and use them!). For example, when implementing a model that is not already pre-programmed as part of the survey package. In that case, we'll need to use `weights` and apply a weighted version of the model to the different estimates. As an example, consider replicating the following linear regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregmodel <- svyglm(api00 ~ ell + meals, design = clus1)\n\n#Create regression table\nregmodel |>\n  modelsummary(estimate = \"{estimate} [{conf.low}, {conf.high}]\", \n               statistic  = NULL, conf_level = 0.95,\n               gof_omit = c(\"IC|R|L|N|F\"),\n               title = \"Classical coefficient estimators\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Classical coefficient estimators</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\">  (1) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 817.182 [776.502, 857.863] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ell </td>\n   <td style=\"text-align:center;\"> −0.509 [−1.219, 0.201] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> meals </td>\n   <td style=\"text-align:center;\"> −3.146 [−3.803, −2.488] </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\nOf course, one way to do that would be via the survey replicates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregmodel2 <- svyglm(api00 ~ ell + meals, design = boot_clus1)\n\n#Create regression table\nregmodel2 |>\n  modelsummary(estimate = \"{estimate} [{conf.low}, {conf.high}]\", \n               statistic  = NULL, conf_level = 0.95,\n               gof_omit = c(\"IC|R|L|N|F\"),\n               title = \"Bootstrapped coefficient estimators\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Bootstrapped coefficient estimators</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\">  (1) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 817.182 [772.839, 861.526] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ell </td>\n   <td style=\"text-align:center;\"> −0.509 [−1.443, 0.425] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> meals </td>\n   <td style=\"text-align:center;\"> −3.146 [−4.028, −2.263] </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nThe `svyrep` package allows us to obtain a `data.frame` of replicate `weights` each of them representing the bootstrapped sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Get the replicate weights\nrep_weights <- weights(boot_clus1)\n```\n:::\n\n\nWe then loop through each of the estimates and compute the variables of interest. In this case, we'll compute and save the coefficients of the regression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefficients <- matrix(NA, nrow = ncol(rep_weights), ncol = 3)\nfor (i in 1:ncol(rep_weights)){\n  model <- lm(api00 ~ ell + meals, data = apiclus1, \n              weights = rep_weights[,i])\n  coefficients[i,] <- coef(model)\n}\n```\n:::\n\n\nFinally we aggregate the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(\"variable\" = names(coef(model)),\n                 \"estimates\" =  coefficients |> colMeans(),\n                 \"sd\" = coefficients |> colSds())\n```\n:::\n\n\n## Confidence intervals\n\nFollowing @arnab2017survey we can estimate both types of confidence intervals for a complex survey design:\n\n### Wald-type confidence intervals\n\nWald-type confidence intervals can be estimated with the variance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\"variable\" = names(coef(model)),\n           \"estimates\" =  colMeans(coefficients),\n           \"var\" = colVars(coefficients),\n           \"ci_low\" = colMeans(coefficients) - t_1_alpha*sqrt(colVars(coefficients)),\n           \"ci_up\" = colMeans(coefficients) + t_1_alpha*sqrt(colVars(coefficients)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     variable   estimates         var     ci_low       ci_up\n1 (Intercept) 814.2897385 414.6276549 774.331790 854.2476873\n2         ell  -0.5015806   0.1838348  -1.342952   0.3397912\n3       meals  -3.1202612   0.1643234  -3.915731  -2.3247913\n```\n:::\n:::\n\n\n### Quantile confidence intervals\n\nQuantile confidence intervals can be estimated with the variance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\"variable\" = names(coef(model)),\n           \"lower_quantile_ci\"  =  coefficients |> colQuantiles(probs = 0.025),\n           \"upper_quantile_ci\"  =  coefficients |> colQuantiles(probs = 0.975))  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     variable lower_quantile_ci upper_quantile_ci\n1 (Intercept)        772.978734       850.8792929\n2         ell         -1.276907         0.4150228\n3       meals         -4.000878        -2.3307806\n```\n:::\n:::\n\n\nAnd that's it!\n\n## Epilogue\n\nA quick note: percentile estimators of the confidence intervals are highly prevalent in the literature and seem widely accepted within the sciences (at least ecology and epidemiology). Albeit, to my knowledge, there is no final proof that these intervals work as intended. [Wu and Rao](https://www.tandfonline.com/doi/epdf/10.1080/01621459.1988.10478591?needAccess=true) argue for a different method (the `t` method). However, [Z. Mashreghi et al]((http://dx.doi.org/10.1214/16-SS113)) states: \"it is not clear that the other types of bootstrap confidence intervals (percentile or t) could be used with either of these methods since they are based on sampling schemes designed to match the variability of the estimator, but not its distribution\". \n\n## References",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}