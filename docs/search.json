[
  {
    "objectID": "posts/regresiones-1/index.html",
    "href": "posts/regresiones-1/index.html",
    "title": "Regresiones (parte 1)",
    "section": "",
    "text": "Adolphe Quetelet fue quien popularizó el uso del método de mínimos cuadrados en las ciencias sociales. Fuente: Miscellaneous Items in High Demand, PPOC, Library of Congress, Public domain, via Wikimedia Commons."
  },
  {
    "objectID": "posts/regresiones-1/index.html#paquetes-y-datos-a-utilizar-en-r",
    "href": "posts/regresiones-1/index.html#paquetes-y-datos-a-utilizar-en-r",
    "title": "Regresiones (parte 1)",
    "section": "Paquetes y datos a utilizar en R",
    "text": "Paquetes y datos a utilizar en R\nA lo largo de esta sección usaremos los siguientes paquetes:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)  #autoplot para diagnósticos\nlibrary(rstanarm)   #para regresión bayesiana\n\n#Siempre que uses tidymodels\ntidymodels_prefer()\n\nEn general usaremos la filosofía tidymodels para combinar los resultados con el tidyverse. Si quieres saber más de tidymodels te recomiendo checar su libro o su página web.\n\nEmbarazo adolescente y pobreza\nPara los datos usaremos la información de Utts y Heckard para determinar si hay una relación entre embarazo adolescente y pobreza.\n\n#Lectura desde sitio web\nurl <- \"https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt\"\nemb_pob <- read_delim(url)\n\nLas variables Brth15to17 y Brth18to19 son las tasas brutas de natalidad por cada 1000 mujeres (en el año 2002) en adolescentes de 15 a 17 años y de 18 a 19 respectivamente. La variable PovPct representa la proporción (%) de la población que vive bajo la línea de pobreza en cada una de las entidades de EEUU (Location). Las variables ViolCrime y TeenBrth no se explican por lo que no las usaremos."
  },
  {
    "objectID": "posts/regresiones-1/index.html#planteamiento-clásico",
    "href": "posts/regresiones-1/index.html#planteamiento-clásico",
    "title": "Regresiones (parte 1)",
    "section": "Planteamiento clásico",
    "text": "Planteamiento clásico\nSi la relación fuera perfecta todos los casos caerían exactamente en una línea recta como sigue:\n\n\n\n\n\ndonde es necesario especificar dos parámetros: el intercepto (el valor que toma cuando la \\(x\\) en este caso PovPct vale cero) y la pendiente (el valor que relaciona por cada unidad de aumento en PovPct cuánto aumenta Brth15to17).\nLa ecuación de la línea está dada por:\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\ndonde \\(\\beta_0\\) es el intercepto y \\(\\beta_1\\) la pendiente. Usando la terminología de arriba:\n\\[\n\\text{Brth15to17} = \\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}\n\\] en particular en ese ejemplo:\n\\[\n\\text{Brth15to17} = 5 + 3\\cdot \\text{PovPct}\n\\]\nLa idea es que el intercepto (\\(\\beta_0\\) ó \\(5\\)) te indica dónde comienza tu línea cuando no tienes \\(x\\)’s (es decir cuando \\(\\text{PovPct} = 0\\)). El intercepto controla la altura de la línea como puedes ver en la siguiente gráfica donde puse varios interceptos distintos:\n\n\n\n\n\npor otro lado la idea de la pendiente (\\(\\beta_1\\) ó \\(3\\)) es retratar cómo cambia la \\(y\\) (en este caso \\(\\text{Brth15to17}\\)) por cada unidad que cambia la \\(x\\) (en este caso \\(\\text{PovPct}\\)). El valor de \\(3\\) por ejemplo indica que por cada aumento en 1 en \\(\\text{PovPct}\\) la variable \\(\\text{Brth15to17}\\) aumenta en \\(3\\). Este cambio es proporcional; es decir si ahora \\(\\text{PovPct}\\) aumenta 4 (por decir algo) \\(\\text{Brth15to17}\\) aumenta \\(3\\times 4 = 12\\) unidades. La siguiente gráfica muestra varias líneas todas comenzando en el mismo intercepto de \\(5\\):\n\n\n\n\n\nComo ya vimos en la primer figura el mundo no es tan perfecto que todo sea una línea recta. Hay un poco de aleatoriedad involucrada (sea por variables no medidas, por errores de medición o porque el mundo no sea determinista). Por lo cual se plantea que en lugar de que la \\(y\\) sea exactamente \\(\\beta_0 + \\beta_1 x\\) planteamos que la \\(y\\) proviene de una variable aleatoria normal donde la media de esa normal es \\(\\beta_0 + \\beta_1 x\\); es decir:\n\\[\ny \\sim \\textrm{Normal}( \\beta_0 + \\beta_1 x, \\sigma^2)\n\\]\no dicho de otra manera:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\sigma^2)\n\\]\ndonde la \\(\\sigma^2\\) es la varianza de dicha normal. Puesto gráficamente lo que esto quiere decir es que si, por ejemplo, el intercepto es \\(5\\) y la pendiente \\(3\\) entonces cada medición de \\(y\\) viene de una normal ligeramente distinta:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(5 + 3\\times \\text{PovPct}, \\sigma^2)\n\\]\n\n\n\n\n\nDicho de otra forma, suponemos que en un mundo perfecto los valores de \\(y\\) (Brth15to17) estarían completamente determinados por los de \\(x\\) (PovPct) mediante la ecuación de la recta. Pero como el mundo no es perfecto entonces la \\(y\\) proviene de una normal con promedio dado por la recta. Los puntos (como puedes ver an la siguiente gráfica) se centran más en torno a los promedios de las normales sin embargo están colocados aleatoriamente pues corresponden a distintas realizaciones de \\(y\\).\n\n\n\n\n\nPor ejemplo si el porcentaje de pobreza (PovPct) es \\(10\\) entonces la normal de la que provienen estos datos es:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\underbrace{5 + 3\\times 10}_{35}, \\sigma^2)\n\\]\nmientras que si el porcentaje en pobreza es \\(20\\) entonces la normal es:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\underbrace{5 + 3\\times 20}_{65}, \\sigma^2)\n\\]\nPor supuesto que no hay nada de especial con el modelo normal y alguien podría elegir otra distribución (por ejemplo una Gamma) y establecer que:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Gamma}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\beta)\n\\]\nEstos modelos son algunos de los lineales generalizados y los discutiremos más adelante. Por ahora nos quedaremos con la idea del modelo dado por:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\sigma^2)\n\\]\n\nNota quizá conoces la regresión lineal bajo la idea clásica de que \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\] donde \\(\\epsilon\\sim\\text{Normal}(0,\\sigma^2)\\) son los errores normales. Esta definición es equivalente a la que damos aquí pues por propiedades aditivas de la normal \\(\\epsilon + \\beta_0 + \\beta_1 x\\) se sigue distribuyendo normal pero con la media ahora dada por lo agregado (\\(\\beta_0 + \\beta_1 x\\)). Las ventajas de esta notación es que un modelo para regresión Poisson es simplemente: \\[\ny \\sim \\textrm{Poisson}\\big(\\exp(\\beta_0 + \\beta_1 x)\\big)\n\\] y un modelo para regresión logística es: \\[\ny \\sim \\textrm{Bernoulli}\\big(\\textrm{logit}(\\beta_0 + \\beta_1 x)\\big)\n\\]"
  },
  {
    "objectID": "posts/regresiones-1/index.html#planteamiento-en-r",
    "href": "posts/regresiones-1/index.html#planteamiento-en-r",
    "title": "Regresiones (parte 1)",
    "section": "Planteamiento en R",
    "text": "Planteamiento en R\nLo que nos toca ahora es programar nuestro modelo para ello seguiremos la filosofía de tidymodels que pretende unificar bajo la misma notación todos los modelos. La notación básica es como sigue:\n\nmodelo %>%\n  set_engine(\"tipo de ajuste\") %>%\n  fit(\"formula a ajustar\", data = tus_datos)\n\nA partir del ajuste se pueden predecir cosas con predict:\n\nmodelo %>%\n  set_engine(\"tipo de ajuste\") %>%\n  fit(\"formula a ajustar\", data = tus_datos) %>%\n  predict()\n\no extraer nuevos datos con extract_fit_engine y tidy:\n\nmodelo %>%\n  set_engine(\"tipo de ajuste\") %>%\n  fit(\"formula a ajustar\", data = tus_datos) %>%\n  extract_fit_engine() %>%\n  tidy()\n\nComencemos con nuestro primer modelo: una regresión lineal clásica dada por:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\sigma^2)\n\\]\nEn R el engine que necesitamos es “lm” que es el clásico:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(Brth15to17 ~ PovPct, data = emb_pob) #Notación y ~ x\n\nNota que a diferencia de Stata, R no arroja demasiados resultados. Podemos usar extract_fit_engine combinado con summary para obtenerlos:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado %>%\n  extract_fit_engine() %>%\n  summary()\n\n\nCall:\nstats::lm(formula = Brth15to17 ~ PovPct, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2275  -3.6554  -0.0407   2.4972  10.5152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.2673     2.5297   1.687    0.098 .  \nPovPct        1.3733     0.1835   7.483 1.19e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.551 on 49 degrees of freedom\nMultiple R-squared:  0.5333,    Adjusted R-squared:  0.5238 \nF-statistic:    56 on 1 and 49 DF,  p-value: 1.188e-09\n\n\no bien con tidy si deseamos nos devuelva una tabla de resultados:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado %>%\n  extract_fit_engine() %>%\n  tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     4.27     2.53       1.69 0.0980       \n2 PovPct          1.37     0.184      7.48 0.00000000119\n\n\nSegún el tipo de regresión que estemos haciendo es el tipo de tabla que regresa tidy (ver ?tidy). En particular, por ejemplo, podemos modificar para que devuelva intervalos de confianza al 90%:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado %>%\n  tidy(conf.int = T, conf.level = 0.90)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic       p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>    <dbl>     <dbl>\n1 (Intercept)     4.27     2.53       1.69 0.0980          0.0260      8.51\n2 PovPct          1.37     0.184      7.48 0.00000000119   1.07        1.68\n\n\nEn este caso, el modelo estima que el intercepto (\\(\\beta_0\\)) es \\(4.27\\) y la pendiente (\\(\\beta_1\\)) es \\(1.37\\). Como son estimadores del verdadero valor se denotan con gorrito: \\(\\hat\\beta_0 = 4.27\\) y \\(\\hat\\beta_1 = 1.37\\).\nPodemos utilizar la función de predict para que el modelo nos muestre cómo cree que son los verdaderos valores en relación a los ajustados:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\npredichos <- modelo_ajustado %>%\n  predict(new_data = emb_pob)\n\nintervalo_predichos <- modelo_ajustado %>%\n  predict(new_data = emb_pob, type = \"pred_int\", level = 0.95)\n\n#Juntamos los predichos con los observados\nobs_y_modelo <- emb_pob %>% \n  cbind(predichos) %>%\n  cbind(intervalo_predichos)\n\n#Graficamos\nggplot(obs_y_modelo) +\n  geom_ribbon(aes(x = PovPct, ymin = .pred_lower, ymax = .pred_upper), \n              fill = \"#003f5c\", size = 1, alpha = 0.5) +\n  geom_point(aes(x = PovPct, y = Brth15to17), color = \"#bc5090\", size = 3) +\n  geom_line(aes(x = PovPct, y = .pred), \n            color = \"#003f5c\", size = 1) +\n  labs(\n    x = \"Porcentaje en pobreza\",\n    y = \"Tasa bruta de natalidad (por cada 1,000 mujeres adolescentes)\",\n    title = \"Relación entre tasa bruta de natalidad en adolescentes\\nde 15 a 19 años y porcentaje en pobreza\"\n  ) +\n  theme_bw()\n\n\n\n\nNada más a ojo no parece que el modelo sea el mejor pues puedes ver que no explica bien la variabilidad (los observados varían mucho respecto al intervalo). La \\(R^2\\), una métrica que explica cuánto de la varianza captura el modelo tampoco es muy buena:\n\nresumen_ajuste <- modelo_ajustado %>%\n  extract_fit_engine() %>%\n  summary()\n\n#R^2 clásica\nresumen_ajuste$r.squared\n\n[1] 0.533328\n\n#R^2 ajustada\nresumen_ajuste$adj.r.squared\n\n[1] 0.523804\n\n\nPodemos checar las diferentes gráficas de diagnóstico:\n\nlibrary(ggfortify)\nautoplot(modelo_ajustado, which = 1:5)\n\n\n\n\nVeamos qué significa cada una de ellas y juguemos un poco con R para irlas modificando.\n\nResiduales contra ajustados\nLos residuales son la diferencia entre el modelo (\\(\\hat{y}\\)) y lo real \\(y\\). En el caso que estábamos trabajando tenemos que con nuestro modelo podemos predecir los valores de Brth15to17 a partir del porcentaje en pobreza PovPct. A los valores predichos por el modelo de Brth15to17 les ponemos un gorro encima y los llamamos: \\(\\widehat{\\text{Brth15to17}}\\). Estos están dados por la siguiente función:\n\\[\n\\widehat{\\text{Brth15to17}} = 4.26 + 1.37 \\cdot \\text{PovPct}\n\\]\npor ejemplo para el porcentaje en pobreza de \\(20.1\\) obtendríamos:\n\\[\n\\widehat{\\text{Brth15to17}} = 4.26 + 1.37 \\cdot \\text{PovPct} = 31.797\n\\]\npor otro lado el verdadero valor de cuando el PovPct es \\(20.1\\) (estado de Alabama) es \\(\\text{Brth15to17} = 31.5\\). La diferencia entre el verdadero valor (\\(\\text{Brth15to17} = 31.5\\)) y el predicho por el modelo (\\(\\widehat{\\text{Brth15to17}} = 31.797\\)) se conoce como el residual. La idea es que en un modelo bueno no debe haber patrones en los residuales (todos deben de flotar en torno al cero pero no mostrar un patrón).\nVeamoslo en nuestra base:\n\n\nCódigo\nobs_y_modelo <- obs_y_modelo %>%\n  mutate(residuales = Brth15to17 - .pred)\n\nggplot(obs_y_modelo) +\n  geom_point(aes(x = .pred, y = residuales), color = \"#ff6361\") +\n  labs(\n    x = \"Valores ajustados (predichos)\",\n    y = \"Residuales\",\n    title = \"Residuales vs ajustados\"\n  ) +\n  theme_bw() +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\")\n\n\n\n\n\nEn esta gráfica el modelo predice mejor rumbo al final que en medio y esto parece estar corroborado por la gráfica del modelo (previa). Nada más para darnos una idea veamos una gráfica de malos residuales y una de buenos\n\n\n\n\n\n\n\nEscala locación\nRepresenta la escala locación contra los residuales estandarizados. La idea de la gráfica es ver que la varianza \\(\\sigma^2\\) del modelo no cambie conforme cambia la \\(x\\) (propiedad de homoscedasticidad). Para ello graficamos los residuales estandarizados dados por los residuales mismos dividos entre su desviación estándar:\n\\[\nr_{\\text{Std}} = \\frac{\\hat{y} - y}{\\text{sd}(\\hat{y} - y)} = \\frac{\\text{Residuales}}{\\text{sd}\\big(\\text{Residuales}\\big)}\n\\]\nestos residuales estandarizados los podemos calcular en R como sigue:\n\nobs_y_modelo <- obs_y_modelo %>%\n  mutate(residuales_std = residuales/sd(residuales))\n\nSi los graficamos contra los valores ajustados no deberíamos de ver ningún patrón:\n\n\nCódigo\nggplot(obs_y_modelo) +\n  geom_point(aes(x = .pred, y = residuales_std), color = \"#ff6361\") +\n  labs(\n    x = \"Valores ajustados (predichos)\",\n    y = \"Residuales estandarizados\",\n    title = \"Escala Locación\"\n  ) +\n  theme_bw() +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\")\n\n\n\n\n\nPodemos ver cómo se ven estos puntos en el modelo ideal vs en un modelo donde la \\(\\sigma^2\\) depende de la \\(x\\):\n\n\n\n\n\n\n\nNormal cuantil cuantil\nLa segunda gráfica corresponde a una gráfica cuantil cuantil. Esta la utilizamos para verificar la hipótesis de normalidad. En una gráfica cuantil cuantil se grafican los cuantiles de los residuales contra los cuantiles teóricos de la normal. Por ejemplo si la hacemos con sólo 4 puntos se vería algo así:\n\n\n\n\n\nPodemos armar una gráfica cuantil cuantil con ggplot2:\n\nggplot(obs_y_modelo, aes(sample = residuales)) + \n  stat_qq(color = \"#ff6361\") + \n  stat_qq_line(color = \"#58508d\") +\n  theme_bw() +\n  labs(\n    x = \"Cuantiles teóricos de la normal\",\n    y = \"Cuantiles observados de los residuales\",\n    title = \"Gráfica qq\"\n  )\n\n\n\n\nLa idea de la gráfica cuantil cuantil es que los puntos sigan la línea lo más posible. Veamos cómo se ve con los datos bien (y los mal)\n\n\n\n\n\n\n\nResiduales contra apalancamiento\nEl apalancamiento representa qué tanto cambia el modelo al quitar una sola observación. Para poner un ejemplo considera los siguientes datos donde hay un valor atípico y selecciono dos puntos de interés en dos colores:\n\n\n\n\n\nVeamos cómo cambia la regresión si dejo todos los puntos, si quito el normal y si quito el influyente:\n\n\n\n\n\nNota que el modelo no cambia prácticamente nada cuando hago la regresión sin el dato que marqué como normal pero cambia mucho cuando quito el que marqué como influyente. La gráfica de residuales contra apalancamiento muestra también el valor extraño:\n\n\n\n\n\nEl apalancamiento mide la influencia de un dato y en R se puede calcular con hatvalues. Los datos con mayor apalancamiento siempre valen la pena checarlos para verificar que todo opera en orden.\n\n\nCódigo\napalancamiento <- modelo_ajustado %>%\n  extract_fit_engine() %>%\n  hatvalues()\n\nobs_y_modelo <- obs_y_modelo %>%\n  cbind(apalancamiento)\n\n#Graficamos residuales contra apalancamiento\nggplot(obs_y_modelo) +\n  geom_point(aes(x = apalancamiento, y = residuales), color = \"#ff6361\") +\n  labs(\n    x = \"Apalancamiento\",\n    y = \"Residuales\",\n    title = \"Residuales vs ajustados\"\n  ) +\n  theme_bw() +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\")\n\n\n\n\n\n\n\nDistancia de Cook\nLa distancia de Cook es un concepto similar al apalancamiento que identifica observaciones influyentes. Aquellos valores con distancia de Cook alta vale la pena revisar. En R podemos usar cooks.distance para calcular la distancia de Cook.\n\n\nCódigo\ndistanciaCook <- modelo_ajustado %>%\n  extract_fit_engine() %>%\n  cooks.distance()\n\nobs_y_modelo <- obs_y_modelo %>%\n  cbind(distanciaCook)\n\n#Graficamos residuales contra apalancamiento\nggplot(obs_y_modelo) +\n  geom_col(aes(x = 1:nrow(obs_y_modelo), y = distanciaCook), fill = \"#ff6361\") +\n  labs(\n    x = \"Observación (número de entrada en la base)\",\n    y = \"Distancia de Cook\",\n    title = \"Distancia de Cook\"\n  ) +\n  theme_bw() \n\n\n\n\n\npodemos ver que en el ejemplo anterior (el de la observación influyente) la distancia de Cook es exagerada, tan exagerada que ni se alcanzan a ver los otros:"
  },
  {
    "objectID": "posts/regresiones-1/index.html#ejercicios",
    "href": "posts/regresiones-1/index.html#ejercicios",
    "title": "Regresiones (parte 1)",
    "section": "Ejercicios",
    "text": "Ejercicios\n\nCorre el siguiente código para generar una base de datos de nombre datLong que contiene \\(4\\) grupos. Para cada grupo genere una regresión lineal de la forma:\n\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\nIdentifica cuáles regresiones sí ajustan bien y cuáles no mediante los gráficos de diagnóstico. Finalmente grafica tus datos \\(x\\) contra \\(y\\) y la regresión para ver que lo hayas hecho bien. ¿Hay alguna forma de corregir alguna de las que no ajusta bien?\n\ndat <- datasets::anscombe\ndatLong <- data.frame(\n    grupo  = rep(1:4, each = 11),\n    x = unlist(dat[,c(1:4)]),\n    y = unlist(dat[,c(5:8)])\n    )\nrownames(datLong) <- NULL\n\n\nLee la base de datos de \\(n = 345\\) niños entre \\(6\\) y \\(10\\) años de Kahn, Michael (2005). Las variables de interés son \\(y = \\text{FEV}\\) el volumen de expiración forzada y \\(x = \\text{edad}\\) en años. Realiza una regresión lineal. Justifica que no se cumple la homocedasticidad mediante una gráfica de escala locación.\nPlantee una regresión lineal usando los datos de esta liga para determinar si el sexo influye en el salario. Ojo en la regresión es necesario incluir otras covariables."
  },
  {
    "objectID": "posts/regresiones-1/index.html#y-si-lo-hacemos-bayesiano",
    "href": "posts/regresiones-1/index.html#y-si-lo-hacemos-bayesiano",
    "title": "Regresiones (parte 1)",
    "section": "¿Y si lo hacemos bayesiano?",
    "text": "¿Y si lo hacemos bayesiano?\nPara hacer la misma regresión lineal pero con estadística bayesiana podemos nada más cambiar el engine:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_bayesiano <- linear_reg() %>% \n  set_engine(\"stan\") %>%\n  fit(Brth15to17 ~ PovPct, data = emb_pob) #Notación y ~ x\n\nY podemos ver el ajuste:\n\nmodelo_bayesiano %>%\n  extract_fit_engine() %>%\n  summary()\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      Brth15to17 ~ PovPct\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 51\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 4.3    2.6  0.9   4.3   7.7  \nPovPct      1.4    0.2  1.1   1.4   1.6  \nsigma       5.7    0.6  4.9   5.6   6.4  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 22.3    1.1 20.9  22.3  23.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  4252 \nPovPct        0.0  1.0  4286 \nsigma         0.0  1.0  3968 \nmean_PPD      0.0  1.0  4057 \nlog-posterior 0.0  1.0  1705 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\no realizar predicciones:\n\npredichos <- modelo_bayesiano %>%\n  extract_fit_engine() %>%\n  predict(new_data = emb_pob)\n\nic <- modelo_bayesiano %>%\n  extract_fit_engine() %>%\n  predictive_interval(newdata = emb_pob, prob = 0.95) #Para bayesiana\n\n#Juntamos los predichos con los observados\nobs_y_modelo <- emb_pob %>% \n  cbind(predichos) %>%\n  cbind(ic)\n\n#Graficamos\nggplot(obs_y_modelo) +\n  geom_ribbon(aes(x = PovPct, ymin = `2.5%`, ymax = `97.5%`), \n              fill = \"#003f5c\", size = 1, alpha = 0.5) +\n  geom_point(aes(x = PovPct, y = Brth15to17), color = \"#ffa600\", size = 3) +\n  geom_line(aes(x = PovPct, y = predichos), \n            color = \"#003f5c\", size = 1) +\n  labs(\n    x = \"Porcentaje en pobreza\",\n    y = \"Tasa bruta de natalidad (por cada 1,000 mujeres adolescentes)\",\n    title = \"Relación entre tasa bruta de natalidad en adolescentes\\nde 15 a 19 años y porcentaje en pobreza\"\n  ) +\n  theme_bw()\n\n\n\n\nPara validación del modelo puedes checar esta página que explica loo (ver paper:\n\nmodelo_bayesiano %>%\n  extract_fit_engine() %>% \n  loo()\n\n\nComputed from 4000 by 51 log-likelihood matrix\n\n         Estimate  SE\nelpd_loo   -161.6 4.2\np_loo         2.4 0.5\nlooic       323.3 8.4\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nasi como su visualización (si el modelo no fuera bueno)\n\nmodelo_bayesiano %>%\n  extract_fit_engine() %>% \n  loo() %>%\n  plot(label_points = TRUE)"
  },
  {
    "objectID": "posts/regresiones-1/index.html#ejercicio",
    "href": "posts/regresiones-1/index.html#ejercicio",
    "title": "Regresiones (parte 1)",
    "section": "Ejercicio",
    "text": "Ejercicio\n\nUtilice las opciones de engine para cambiar el prior_intercept del intercepto a una t de Student y el prior de los coeficientes a una Laplace. ¿Cambia mucho el resultado?"
  },
  {
    "objectID": "posts/Navidad/index.html#sec-spanish-tutorial",
    "href": "posts/Navidad/index.html#sec-spanish-tutorial",
    "title": "Navidad/Christmas @ #rstats",
    "section": "Tutorial en español",
    "text": "Tutorial en español\nEsta Navidad las Rladies de Querétaro hicieron un concurso para realizar un árbol de Navidad con R y decidí participar. Esta vez no quería hacer el típico arbolito con ggplot2 así que decidí probar otra tecnología: rayrender.\nrayrender es una librería para crear imágenes 3D mediante trazado de rayos (raytracing). No es la mejor opción open source para la tarea pero ¡hey está en R!\nLa forma en la que rayrender funciona es consturyendo una escena y a partir de ésta agregar objetos a la escena. Podemos empezar con una escena vacía de estudio fotográfico:\n\nlibrary(rayrender)\nscene_test <- generate_studio(depth = 0.2, \n                         material = diffuse(checkercolor = \"red\"))\n\nPara poder ver previsualizar la escena se utiliza render_scene:\n\nrender_scene(scene_test, \n             samples = 100, #Muestras (+ muestras menos ruido)\n             preview = T,   #Si quieres previsualizar la escena antes de calcular\n             parallel = T   #Si realizar el cómputo en paralelo\n             )\n\n\n\n\n\n\n\n\n\n\nSobre la escena podemos agregar objetos con add_object. En particular podemos agregar un cilindro para el tronco especificando sus coordenadas así como el material:\n\nscene_test <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nHay diferentes materiales, por ejemplo metal, cabello hair, luz light, cristal dielectric, etc (la lista completa en el apartado materials) por lo que si quisiéramos hacer nuestro árbol de cristal bastaría cambiar el material:\n\nscene_metal <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = metal(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nPodemos cambiar la perspectiva de la cámara ajustando manualmente y dando p para obtener las coordenadas y luego imputarlas en el render:\n\nrender_scene(scene_metal, \n             samples = 100, \n             preview = T, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), #Dónde está la cámara\n             lookat   = c(1, 5, 0), #Dónde está viendo la cámara\n             aperture = 0.5, #Apertura \n             fov = 17,\n             focal_distance = 77.66, #Distancia focal\n             iso = 400, #Sensibilidad \"del rollo fotográfico\" a la luz \n             clamp_value = 10\n             )\n\n\n\n\n\n\n\n\n\n\nSobre nuestro árbol podemos agregar conos verdes. Agregamos varios conos en un loop para darle mayor figura. Lo pondremos sobre un nuevo fondo (blanco) y con el tronco que hicimos previamente usando diffuse:\n\nscene <- generate_studio(depth = 0.2) |>\n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  ) \n\n#Conos\nfor (i in seq(1, 10, length.out = 20)){\n  scene <- scene |> \n    add_object(\n      cone(\n        start  = c(0, 6 - i/2, 0),\n        end    = c(0, 6 - (i/2 - 1), 0),\n        radius = i/3,\n        material = diffuse(color = \"darkgreen\")\n      )\n    )\n}\n\n\n\n\n\n\n\n\n\n\nFinalmente agregamos esferas luminosas de dos colores distintos: amarillas y rojas sobre las superficies de los conos aleatoriamente:\n\nset.seed(27522)\n#Esferas luminosas\nfor (i in seq(1, 10, length.out = 20)){\n  #Esferas rojas\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    #Altura de la esfera\n        x = cos(k)*i/3, #Coordenadas polares para superficie de cono\n        z = sin(k)*i/3,\n        material = light(color = \"red\", intensity = 5)\n      )\n    )\n  }\n  #Esferas amarillas\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    #Altura de la esfera\n        x = cos(k)*i/3, #Coordenadas polares para superficie de cono\n        z = sin(k)*i/3,\n        material = light(color = \"#f8d568\", intensity = 5)\n      )\n    )\n  }\n}\n\n\n\n\n\n\n\n\n\n\nLa estrella se agrega hasta arriba con un polígono extruded_polygon que permite diseñar figuras.\n\n#Adaptado de la estrella de https://www.rayrender.net/index.html\nangulos  <- seq(0, 2*pi, length.out = 11)\nx        <- rev(c(rep(c(1,0.5), 5), 1)) * cos(angulos)\nz        <- rev(c(rep(c(1,0.5), 5), 1)) * sin(angulos)\npoligono <- data.frame(x = x, z = z)\nestrella <- rbind(poligono, 0.8*poligono)\n\n#Agregamos la estrella luminosa a la escena\nscene <- scene |>\n  add_object(\n    extruded_polygon(\n      estrella,\n      top = -0.5,\n      bottom = -1,\n      y = 7,    #Altura\n      z = 0.75, #Centrar\n      angle = c(90, 0, 90),\n      material = light(color = \"white\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nUna vez que está listo nuestro árbol nos preparamos para renderizarlo con suficientes muestras para eliminar todo el ruido:\n\n\n\n\n\n\nWarning\n\n\n\nÉste proceso tarda varias horas\n\n\n\npng(\"arbolito.png\")\nrender_scene(scene, \n             width = 500,    #Ancho en pixeles\n             height = 500,   #Alto en pixeles\n             samples = 1000, #Suficientes muestras!\n             preview = F, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), \n             lookat   = c(1, 5, 0), \n             aperture = 0.5, \n             fov = 17,\n             focal_distance = 77.66,\n             iso = 400, \n             clamp_value = 10\n             )\ndev.off()\n\n\n\n\n\n\n\n\n\n\nFinalmente al arbolito le agregamos texto de Feliz Navidad\n\nlibrary(png)\nlibrary(showtext)\nlibrary(cowplot)\nlibrary(ggplot2)\n\n#Descarga de la fuente Passions Conflict\nfont_add_google(\"Passions Conflict\", \"pconflict\")\nshowtext_auto()\n\n#Leemos la imagen\narbol <- readPNG(\"arbolito.png\")\n\ndrawplot <- ggdraw() +\n  annotation_raster(arbol, xmin = 0, ymin = 0, xmax = 1, ymax = 1) +\n  geom_text(aes(x = 0.5, y = 0.1, label = \"Feliz Navidad\"), color = \"white\",\n            family = \"pconflict\", size = 15) +\n  geom_text(aes(x = 0.5, y = 0.95,\n                label = \"@RodZepeda | rodrigozepeda.github.io/Statisticats/posts/Navidad\"),\n            color = \"gray75\",\n            size = 3) \nggsave(\"arbolito_navidad_2022_es.png\", drawplot, dpi = 100, width = 500, height = 500, units = \"px\")"
  },
  {
    "objectID": "posts/Navidad/index.html#sec-english-tutorial",
    "href": "posts/Navidad/index.html#sec-english-tutorial",
    "title": "Navidad/Christmas @ #rstats",
    "section": "Tutorial in English",
    "text": "Tutorial in English\n\n\n\n\n\n\nTutorial en español\n\n\n\nPara el tutorial en español ve a Section 1.\n\n\nThis Christmas Rladies Querétaro created a contest. One was to build a Christmas tree in R. I decided to participate. This time I didn’t want to do the typical ggplot2 tree so I decided to test another technology: rayrender.\nrayrender is a library to create 3D images using raytracing. This is not the best open source option for this task, but hey, it’s in R!\nThe way rayrender works is by building a scene and adding objects to it. We can start with an empty study scene:\n\nlibrary(rayrender)\nscene_test <- generate_studio(depth = 0.2, \n                         material = diffuse(checkercolor = \"red\"))\n\nTo preview the scene use render_scene:\n\nrender_scene(scene_test, \n             samples = 100, #More samples less noise\n             preview = T,   #To preview the scene (before calculating)\n             parallel = T   #Compute in parallel for extra speed\n             )\n\n\n\n\n\n\n\n\n\n\nWe can add different objects onto the scene with add_object. In particular, we can add a cylinder for the tree trunk as well as its coordinates:\n\nscene_test <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nThere are different materials (e.g. metal, hair, light, (crystal) dielectric, etc see materials). If we wish to make our tree metal we can just change the material:\n\nscene_metal <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = metal(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nWe can shift the camera’s perspective by manually adjusting and obtain the coordinates with p. These coordinates can be inputed into the render:\n\nrender_scene(scene_metal, \n             samples = 100, \n             preview = T, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), #Where camera is\n             lookat   = c(1, 5, 0), #What is camera watching\n             aperture = 0.5, \n             fov = 17,\n             focal_distance = 77.66, \n             iso = 400, #Sensitivity to light\n             clamp_value = 10\n             )\n\n\n\n\n\n\n\n\n\n\nWe can add green cones to our tree. We do that with a for loop. We’ll put that tree over a white background and with the previous trunk we had using diffuse:\n\nscene <- generate_studio(depth = 0.2) |>\n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  ) \n\n#Cones\nfor (i in seq(1, 10, length.out = 20)){\n  scene <- scene |> \n    add_object(\n      cone(\n        start  = c(0, 6 - i/2, 0),\n        end    = c(0, 6 - (i/2 - 1), 0),\n        radius = i/3,\n        material = diffuse(color = \"darkgreen\")\n      )\n    )\n}\n\n\n\n\n\n\n\n\n\n\nFinally, we add light spheres of two colors: yellow and red. These go randomly over the cone’s surface.\n\nset.seed(27522)\nfor (i in seq(1, 10, length.out = 20)){\n  #Red spheres\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    #Sphere height\n        x = cos(k)*i/3, #Polar coordinates for the cone's surface\n        z = sin(k)*i/3,\n        material = light(color = \"red\", intensity = 5)\n      )\n    )\n  }\n  #yellow spheres\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    \n        x = cos(k)*i/3, \n        z = sin(k)*i/3,\n        material = light(color = \"#f8d568\", intensity = 5)\n      )\n    )\n  }\n}\n\n\n\n\n\n\n\n\n\n\nWe add an extruded_polygon for the star:\n\n#Adapted from the star at https://www.rayrender.net/index.html\nangulos  <- seq(0, 2*pi, length.out = 11)\nx        <- rev(c(rep(c(1,0.5), 5), 1)) * cos(angulos)\nz        <- rev(c(rep(c(1,0.5), 5), 1)) * sin(angulos)\npoligono <- data.frame(x = x, z = z)\nestrella <- rbind(poligono, 0.8*poligono)\n\n#Add the luminous star\nscene <- scene |>\n  add_object(\n    extruded_polygon(\n      estrella,\n      top = -0.5,\n      bottom = -1,\n      y = 7,    #Altura\n      z = 0.75, #Centrar\n      angle = c(90, 0, 90),\n      material = light(color = \"white\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nOnce our tree is ready we render it with enough samples to eliminate the noise:\n\n\n\n\n\n\nWarning\n\n\n\nThis process takes several hours\n\n\n\npng(\"arbolito.png\")\nrender_scene(scene, \n             width = 500,    #Pixel width\n             height = 500,   #Pixel height\n             samples = 1000, #Enough samples\n             preview = F, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), \n             lookat   = c(1, 5, 0), \n             aperture = 0.5, \n             fov = 17,\n             focal_distance = 77.66,\n             iso = 400, \n             clamp_value = 10\n             )\ndev.off()\n\n\n\n\n\n\n\n\n\n\nFinally we add Merry Christmas text to the tree\n\nlibrary(png)\nlibrary(showtext)\nlibrary(cowplot)\nlibrary(ggplot2)\n\n#Download Passions Conflict from google fonts\nfont_add_google(\"Passions Conflict\", \"pconflict\")\nshowtext_auto()\n\n#Leemos la imagen\narbol <- readPNG(\"arbolito.png\")\n\ndrawplot <- ggdraw() +\n  annotation_raster(arbol, xmin = 0, ymin = 0, xmax = 1, ymax = 1) +\n  geom_text(aes(x = 0.5, y = 0.1, label = \"Merry Christmas\"), color = \"white\",\n            family = \"pconflict\", size = 15) +\n  geom_text(aes(x = 0.5, y = 0.95,\n                label = \"@RodZepeda | rodrigozepeda.github.io/Statisticats/posts/Navidad\"),\n            color = \"gray75\",\n            size = 3) \nggsave(\"arbolito_navidad_2022_en.png\", drawplot, dpi = 100, width = 500, height = 500, units = \"px\")"
  },
  {
    "objectID": "posts/bootstraping-a-survey-regression/index.html",
    "href": "posts/bootstraping-a-survey-regression/index.html",
    "title": "Bootstrapping a survey regression",
    "section": "",
    "text": "pacman::p_load(survey, svrep, matrixStats, modelsummary)\n\nThe main idea behind bootstrap is that resampling the random sample allows us to generate replicates of the sampling process itself. Using these re-sampled data, we can generate our desired estimators. These estimators will be asymptotically equivalent to the desired quantities.\n\n\nAs an example, consider a random sample of n = 1000 samples from a \\(Normal(0,1)\\) distribution: \\[\n\\{X_1, X_2, \\dots, X_n\\} \\text{ with } X_i\\sim \\text{Normal}(0,1)\n\\]\nIn R this can be obtained as:\n\nsamples <- rnorm(1000)\n\nOne can estimate the mean with the classical estimator, \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\):\n\nmean(samples)\n\n[1] 0.01726104\n\n\nIn R this has already been programmed as:\n\nt.test(samples)$conf.int\n\n[1] -0.04433005  0.07885213\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThe bootstrap estimate relies on going over several samples and calculating the mean for each of the resamples:\n\nresample_means <- rep(NA, 1000) #Save the mean of the re-samples\nfor (i in 1:1000){\n  resample <- sample(samples, size = 1000, replace = TRUE)\n  resample_means[i] <- mean(resample)\n}\n\n#Bootstrap estimate:\nmean(resample_means)\n\n[1] 0.01633088\n\n\nThe Wald-type confidence intervals at level \\((1-\\alpha)\\times 100\\%\\) are given by: \\[\n\\bar{X}\\pm t_{1-\\alpha/2}\\sqrt{\\widehat{\\textrm{Var}}_{\\text{Boot}}(\\bar{X})}\n\\] where the unbiased estimator of the variance is:\n\\[\n\\widehat{\\textrm{Var}}_{\\text{Boot}}(\\bar{X}) = \\sum\\limits_{i = 1}^{n} (\\bar{X}_{\\text{Boot},i} - \\bar{X})^2\n\\]\nwhere \\(\\bar{X}_{\\text{Boot},i}\\) is the estimate of the mean in the \\(i\\)-th bootstrap sample.\nIn R, these confidence intervals can be computed as:\n\nmu_estim  <- mean(resample_means)\nvar_estim <- var(resample_means)\nt_1_alpha <- qt(1 - 0.05/2, df = 1000)\nci_wald   <- c(\"Lower\" = mu_estim - t_1_alpha*sqrt(var_estim), \n               \"Upper\" = mu_estim + t_1_alpha*sqrt(var_estim))\n\n\nNotice that the formula is not divided by \\(n\\) as we are estimating the standard error of the variance for the mean statistic and not of the \\(X_i\\). Intuitively, it makes sense not to divide by the number of bootstrap samples. If we did we could then artificially decrease uncertainty around estimates just by taking more boostrap samples without gathering more data!\n\nAnother type of confidence intervals (not recommended) is the percentile confidence interval given by the \\(\\alpha/2\\) and \\(1 - \\alpha/2\\) sample-percentiles of the bootstrap sample:\n\\[\n[\\bar{X}_{\\text{Boot}, (\\alpha/2)}, \\bar{X}_{\\text{Boot}, (1 - \\alpha/2)}]\n\\] In R:\n\n#Bootstrap estimate:\nquantile(resample_means, c(0.025, 0.975))\n\n       2.5%       97.5% \n-0.04485072  0.07992000 \n\n\nIn this particular case, they all are excellent estimates for our quantity of interest:"
  },
  {
    "objectID": "posts/bootstraping-a-survey-regression/index.html#generating-bootstrap-samples-for-surveys",
    "href": "posts/bootstraping-a-survey-regression/index.html#generating-bootstrap-samples-for-surveys",
    "title": "Bootstrapping a survey regression",
    "section": "Generating bootstrap samples for surveys",
    "text": "Generating bootstrap samples for surveys\nThere are already pre-programmed functions in the survey package that allow users to calculate some quantities of interest via bootstrap. In this example, we’ll go through the first part of the survey example vignette but with bootstrap. We’ll load the data and setup the survey design, closely following the vignette:\n\nlibrary(survey)\ndata(api)\nclus1 <- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)\n\nThen, we’ll estimate the mean:\n\nsvymean(~api00, clus1)\n\n        mean     SE\napi00 644.17 23.542\n\n\nTo change into bootstrap mode one has only to replicate the design\n\nboot_clus1 <- as_bootstrap_design(clus1, replicates = 1000)\nsvymean(~api00, boot_clus1) #Mean but now using 1000 re-samples\n\n        mean     SE\napi00 644.17 22.599\n\n\nNotice that we had to use the design to generate the replicates. This is an important thing to keep in mind as for complex surveys one cannot simply resample without considering the design Mashreghi, Haziza, and Léger (2016)."
  },
  {
    "objectID": "posts/bootstraping-a-survey-regression/index.html#playing-with-the-bootstrapped-sample",
    "href": "posts/bootstraping-a-survey-regression/index.html#playing-with-the-bootstrapped-sample",
    "title": "Bootstrapping a survey regression",
    "section": "Playing with the bootstrapped sample",
    "text": "Playing with the bootstrapped sample\nNow, what happens if we actually need to obtain the replicates (and use them!). For example, when implementing a model that is not already pre-programmed as part of the survey package. In that case, we’ll need to use weights and apply a weighted version of the model to the different estimates. As an example, consider replicating the following linear regression model:\n\nregmodel <- svyglm(api00 ~ ell + meals, design = clus1)\n\n#Create regression table\nregmodel |>\n  modelsummary(estimate = \"{estimate} [{conf.low}, {conf.high}]\", \n               statistic  = NULL, conf_level = 0.95,\n               gof_omit = c(\"IC|R|L|N|F\"),\n               title = \"Classical coefficient estimators\")\n\n\n\nClassical coefficient estimators\n \n  \n      \n     (1) \n  \n \n\n  \n    (Intercept) \n    817.182 [776.502, 857.863] \n  \n  \n    ell \n    −0.509 [−1.219, 0.201] \n  \n  \n    meals \n    −3.146 [−3.803, −2.488] \n  \n\n\n\n\n\nOf course, one way to do that would be via the survey replicates:\n\nregmodel2 <- svyglm(api00 ~ ell + meals, design = boot_clus1)\n\n#Create regression table\nregmodel2 |>\n  modelsummary(estimate = \"{estimate} [{conf.low}, {conf.high}]\", \n               statistic  = NULL, conf_level = 0.95,\n               gof_omit = c(\"IC|R|L|N|F\"),\n               title = \"Bootstrapped coefficient estimators\")\n\n\n\nBootstrapped coefficient estimators\n \n  \n      \n     (1) \n  \n \n\n  \n    (Intercept) \n    817.182 [772.839, 861.526] \n  \n  \n    ell \n    −0.509 [−1.443, 0.425] \n  \n  \n    meals \n    −3.146 [−4.028, −2.263] \n  \n\n\n\n\n\nThe svyrep package allows us to obtain a data.frame of replicate weights each of them representing the bootstrapped sample.\n\n#Get the replicate weights\nrep_weights <- weights(boot_clus1)\n\nWe then loop through each of the estimates and compute the variables of interest. In this case, we’ll compute and save the coefficients of the regression:\n\ncoefficients <- matrix(NA, nrow = ncol(rep_weights), ncol = 3)\nfor (i in 1:ncol(rep_weights)){\n  model <- lm(api00 ~ ell + meals, data = apiclus1, \n              weights = rep_weights[,i])\n  coefficients[i,] <- coef(model)\n}\n\nFinally we aggregate the data:\n\ndf <- data.frame(\"variable\" = names(coef(model)),\n                 \"estimates\" =  coefficients |> colMeans(),\n                 \"sd\" = coefficients |> colSds())"
  },
  {
    "objectID": "posts/bootstraping-a-survey-regression/index.html#epilogue",
    "href": "posts/bootstraping-a-survey-regression/index.html#epilogue",
    "title": "Bootstrapping a survey regression",
    "section": "Epilogue",
    "text": "Epilogue\nA quick note: percentile estimators of the confidence intervals are highly prevalent in the literature and seem widely accepted within the sciences (at least ecology and epidemiology). Albeit, to my knowledge, there is no final proof that these intervals work as intended. Wu and Rao argue for a different method (the t method). However, Z. Mashreghi et al states: “it is not clear that the other types of bootstrap confidence intervals (percentile or t) could be used with either of these methods since they are based on sampling schemes designed to match the variability of the estimator, but not its distribution”."
  },
  {
    "objectID": "posts/memoise/index.html",
    "href": "posts/memoise/index.html",
    "title": "FasteR functions with memoise",
    "section": "",
    "text": "Memoisation is a technique for speeding up functions via memorization of previously calculated results. To better explain the idea let’s consider a recursive formulation of the Fibonacci sequence:\n\\[\nf(n) = f(n- 1) + f(n-2)\n\\]\nwith \\(f(1) = 1 = f(2)\\). 1\nAn implementation of the function is given by:\n\nfibonacci <- function(n){\n  if(n <= 2){\n    return(1)\n  } else {\n    return(\n      fibonacci(n - 1) + fibonacci(n - 2)\n    )\n  }\n}\n\nWe can calculate the time it takes to estimate the number up to 20:\n\nmicrobenchmark::microbenchmark(fibonacci(20))\n\nUnit: milliseconds\n          expr      min       lq    mean   median       uq      max neval\n fibonacci(20) 4.633172 5.810971 6.92568 6.418399 7.803796 17.74069   100\n\n\nLarger values (like fibonacci(100)) start taking a lot of time. And that’s because fibonacci(100) estimates the same values several times! To illustrate this point, consider fibonacci(5). You can see that fibonacci(3) is estimated twice: once under fibonacci(5) itself and one under fibonacci(4). This is extremely inefficient!\n\n\n\n\ngraph TD\n    f5[fibonacci 5] --> f4[fibonacci 4]\n    f5[fibonacci 5] --> f3[fibonacci 3]\n    f3 --> f13[fibonacci 1]\n    f3 --> f23[fibonacci 2]\n    f4 --> f32[fibonacci 3]\n    f4 --> f2[fibonacci 2]\n    f32 --> f21[fibonacci 2]\n    f32 --> f11[fibonacci 1]\n\n\n\n\n\n\n\n\nWe can calculate how many times the function fibonacci is called when estimating different numbers. In theory it should scale linearly i.e. fibonacci(20) should only calculate fibonacci(1), fibonacci(2), etc up to fibonacci(19) once. Hence the function should be called at most 20 times. However this isn’t the case:\n\ncalls_f   <- 0\nfibonacci_calls <- function(n){\n  calls_f <<- calls_f + 1\n  if(n <= 2){\n    return(1)\n  } else {\n    return(\n      fibonacci_calls(n - 1) + fibonacci_calls(n - 2)\n    )\n  }\n}\ninvisible(fibonacci_calls(20))\n\ncat(paste(\"fibonacci_calls was called\",calls_f,\"times\"))\n\nfibonacci_calls was called 13529 times\n\n\nThe algorithm is extremely inefficient because it doesn’t remember when estimating fibonacci_calls(8) that it already has estimated fibonacci_calls(7) during its estimation of fibonacci_calls(9). Memoisation is a solution for this forgetfulness."
  },
  {
    "objectID": "posts/memoise/index.html#sec-hard-coded",
    "href": "posts/memoise/index.html#sec-hard-coded",
    "title": "FasteR functions with memoise",
    "section": "Hard coded memoisation",
    "text": "Hard coded memoisation\nAs we have seen, fibonacci(20) calls the fibonacci function 13,529 times. This inefficiency could be saved if the function could memorize that it has already calculated the previous results. That is the utility of the memoisation trick. To save these memorizations (memoise!), let’s create a global vector where we’ll keep the previous results of the fibonacci function. That is, the j-th entry of our vector will correspond to fibonacci(j).\n\n#Fibonacci cache vector up til fibonacci 1000 \n#fibonacci_cache[j] = fibonacci(j)\nfibonacci_cache <- rep(NA, 1000)\n\n#fibonacci_cache[1] = fibonacci(1) and fibonacci_cache[2] = fibonacci(2) \nfibonacci_cache[1:2] <- c(1, 1)\n\nmemoised_fibonacci <- function(n){\n  #Only calculate the values we haven't previously estimated\n  if (is.na(fibonacci_cache[n])){\n    fibonacci_cache[n] <<- memoised_fibonacci(n - 1) + memoised_fibonacci(n - 2)\n  } \n  return(fibonacci_cache[n])\n}\n\n\n\n\nLet’s compare the speed of the previous function with this one:\n\nmicrobenchmark::microbenchmark(fibonacci(20), memoised_fibonacci(20))\n\nUnit: nanoseconds\n                   expr     min        lq       mean    median      uq     max\n          fibonacci(20) 3942361 4796700.5 5059247.18 4838703.0 4977222 8489579\n memoised_fibonacci(20)     398     589.5    2730.04    1642.5    3855   28640\n neval\n   100\n   100\n\n\nThis speed-up happens because the new memoised_fibonacci only calculates the value 18 times! In contrast with the previous 13,529. For any custom function you build you can memoise this way ooooor you can let the memoise package do it for you."
  },
  {
    "objectID": "posts/memoise/index.html#the-memoise-package",
    "href": "posts/memoise/index.html#the-memoise-package",
    "title": "FasteR functions with memoise",
    "section": "The memoise package",
    "text": "The memoise package\nThe memoise package does exactly what we did in the previous section by automatically memoising functions (with arguments that aren’t necessarily numbers). It sets limits to the memory (our fibonacci_cache) as well as the amount of time a function will remember previous results (time limit). To memoise a function you just need to call memoise over it:\n\nlibrary(memoise)\nmfibo <- function(n){\n  if(n <= 2){\n    return(1)\n  } else {\n    return(\n      mfibo(n - 1) + mfibo(n - 2) #It's important to call this with the memoized name\n    )\n  }\n}\nmfibo <- memoise(mfibo) #Memoization line\n\n\n\n\nNote that this memoisation has additional overhead over the memoisation we did in Section 2 because memoise works even for non numeric arguments (which would have failed in our vectorized example). However the speed-up over the original is still there:\n\nmicrobenchmark::microbenchmark(fibonacci(20), mfibo(20), memoised_fibonacci(20)) \n\nUnit: nanoseconds\n                   expr     min        lq       mean  median        uq      max\n          fibonacci(20) 5291457 6031574.5 7138627.87 6599172 7929635.0 14128101\n              mfibo(20)   58632   72422.5  151487.39  139772  190614.5   569502\n memoised_fibonacci(20)     606    1206.0    5186.56    3470    7506.5    39376\n neval\n   100\n   100\n   100\n\n\nThe memoise package uses cachem which allows for fine control over where the previous results of the function. You can substitute the #Memoization line in the previous code for this memoise with finer control.\n\n#Set memory to 100MB and time to memorizing only for 15 minutes\ncm    <- cachem::cache_mem(max_size = 100 * 1024^2, max_age = 15 * 60)\nmfibo <- memoise(mfibo, cache = cm)\n\nTo keep previous computations across different R sessions you can cache directly to disk (slower) instead of memory:\n\ncm    <- cachem::cache_disk(max_size = 100 * 1024^2, max_age = 15 * 60)\nmfibo <- memoise(mfibo, cache = cm)"
  },
  {
    "objectID": "posts/memoise/index.html#and-in-other-languages",
    "href": "posts/memoise/index.html#and-in-other-languages",
    "title": "FasteR functions with memoise",
    "section": "And in other languages?",
    "text": "And in other languages?\nYou can also memoise in the most recent versions of Python either vie the built in functools or the memoization project. In Julia you can Memoize.jl. Let me know if you are interested in an entry for any of these."
  },
  {
    "objectID": "posts/characteristic-function-definetti/index.html#a-geometric-interpretation-of-the-inversion-theorem",
    "href": "posts/characteristic-function-definetti/index.html#a-geometric-interpretation-of-the-inversion-theorem",
    "title": "An intuitive (geometric) interpretation of the characteristic function of a random variable",
    "section": "A geometric interpretation of the inversion theorem",
    "text": "A geometric interpretation of the inversion theorem\nThe characteristic function \\(\\phi(t)\\) corresponds to the path of the barycentre of the circumference as the points around it rotate by \\(t\\). This represents a unique signature2:\n\n\n\n\n\n\nThe path of \\(\\phi(t)\\) as reconstructed from the three \\(x_k\\) represents the random variable’s unique signature.\n\n\nBefore going into the inverse transform there remains a second point of interest. As was seen in the previous animation, a fixed \\(x_k\\) in \\(e^{itx_k}\\) represents the “speed” at which the particle (that pulls the barycenter with weight \\(p_k\\)) travels around the circumference. Intuitively if you think of \\(t\\) moving from \\(-\\pi\\) to \\(\\pi\\), then \\(x_k = 1\\) represents the reference speed for traveling around the circumference. Any \\(x_k > 1\\) will result in a path that goes around faster and travels more than a cycle while \\(x_k < 1\\) will not complete a full cycle in the \\([-\\pi,\\pi]\\) period.\nNow we can go and interpret the inverse fourier transform. By taking \\(\\phi(t)\\) and multiplying it by \\(e^{-itx_k}\\) (for a fixed \\(x_k\\)) we project outwards from the barycenter. As the angle \\(t\\) varies from \\(-\\pi\\) to \\(\\pi\\), the barycenter is pulled by a force \\(p_x\\) that moves at speed \\(x\\) around the circumference.\n\n\n\n\n\n\nBarycenter image 2\n\n\nAveraging all around the circumference (i.e. from \\([-\\pi,\\pi]\\)) returns the average pull that moves at speed \\(x\\) and pulls the barycentre in its direction. The normalization of this (dividing by the \\(2\\pi\\) of the circumference’s length) corresponds to \\(\\mathbb{P}(X =x)\\).\nHence the inversion theorem for discrete random variables follows:\n\\[\n\\mathbb{P}(X = x_k) = \\frac{1}{2\\pi} \\int\\limits_{-\\pi}^{\\pi} e^{-itx_k} \\phi(t) dt.\n\\]\nSo, what do you think? Has the geometric interpretation helped you in understanding the characteristic function? Let me know!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hola, soy Rodrigo Zepeda-Tello.",
    "section": "",
    "text": "I’m a mathematician turned statistician. I’ve been modeling (with math… ehem) epidemics for the last 10 years. I have worked with differential equations (stochastic, ordinary, etc), abstract networks, bayesian statistics ~(and frequentist)~, random forests, neural networks, time series and survey designs. I also lecture a course in Applied Statistics and give short courses in R. In this blog I post stuff I find interesting (and course notes) oftentimes in English and sometimes in Spanish.\nFeel free to contact me on Twitter, Github, and ResearchGate.\nIf you like my work consider:\n\nhiring me to consult for your project,\ncommissioning a course,\nbuying me a coffee or a book,\nsubscribing to get the latest posts directly in your email.\n\n\n\nSoy un matemático que se volvió estadístico. He estado modelando (con matemáticas… ehem) epidemias desde hace 10 años. He trabajado con ecuaciones diferenciales (estocásticas, ordinarias, etc), redes abstractas, estadística bayesiana ~(y frecuentista)~, bosques aleatorios, redes neuronales, series de tiempo y diseños de encuestas. También doy clases de estadística aplicada y cursos cortos de R. En este blog escribo cosas que me parecen interesantes (sí como notas de cursos) a veces en inglés y a veces en español.\nContáctame en Twitter, Github y ResearchGate.\nSi te gusta mi trabajo considera:\n\ncontratarme como consultor de tu proyecto,\ncomisionar un curso,\ncomprarme un cafecito o un libro,\nsuscribirte para tener las entradas más recientes directamente en tu correo"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About | Acerca de",
    "section": "",
    "text": "Applied mathematician. I spend my time modelling (but with math). Some years ago I became a statistician by mistake: haven’t been able to leave the club since."
  },
  {
    "objectID": "about.html#español",
    "href": "about.html#español",
    "title": "About | Acerca de",
    "section": "Español:",
    "text": "Español:\nMatemático aplicado. Me la vivo modelando (pero con matemáticas). Hace algunos años me convertí en un estadístico por error: no he podido salir de este club desde entonces."
  },
  {
    "objectID": "posts/bootstraping-a-survey-regression/index.html#references",
    "href": "posts/bootstraping-a-survey-regression/index.html#references",
    "title": "Bootstrapping a survey regression",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/bootstraping-a-survey-regression/index.html#confidence-intervals",
    "href": "posts/bootstraping-a-survey-regression/index.html#confidence-intervals",
    "title": "Bootstrapping a survey regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nFollowing Arnab (2017) we can estimate both types of confidence intervals for a complex survey design:\n\nWald-type confidence intervals\nWald-type confidence intervals can be estimated with the variance:\n\ndata.frame(\"variable\" = names(coef(model)),\n           \"estimates\" =  colMeans(coefficients),\n           \"var\" = colVars(coefficients),\n           \"ci_low\" = colMeans(coefficients) - t_1_alpha*sqrt(colVars(coefficients)),\n           \"ci_up\" = colMeans(coefficients) + t_1_alpha*sqrt(colVars(coefficients)))\n\n     variable   estimates         var     ci_low       ci_up\n1 (Intercept) 814.2897385 414.6276549 774.331790 854.2476873\n2         ell  -0.5015806   0.1838348  -1.342952   0.3397912\n3       meals  -3.1202612   0.1643234  -3.915731  -2.3247913\n\n\n\n\nQuantile confidence intervals\nQuantile confidence intervals can be estimated with the variance:\n\ndata.frame(\"variable\" = names(coef(model)),\n           \"lower_quantile_ci\"  =  coefficients |> colQuantiles(probs = 0.025),\n           \"upper_quantile_ci\"  =  coefficients |> colQuantiles(probs = 0.975))  \n\n     variable lower_quantile_ci upper_quantile_ci\n1 (Intercept)        772.978734       850.8792929\n2         ell         -1.276907         0.4150228\n3       meals         -4.000878        -2.3307806\n\n\nAnd that’s it!"
  },
  {
    "objectID": "posts/intensidad-espacial-mx/index.html",
    "href": "posts/intensidad-espacial-mx/index.html",
    "title": "Tutorial: Crear mapas de calor (intensidad) para México",
    "section": "",
    "text": "En esta entrada aprenderemos a hacer un mapa de calor como el que sigue:\n\n\n\nMapa de calor de la Ciudad de México muestra mayor intensidad al norte\n\n\nUsaremos los siguientes paquetes para generar el mapa:\n\n# Lectura de los paquetes\npacman::p_load(tidyverse, sf, spatstat, stars, wesanderson)"
  },
  {
    "objectID": "posts/intensidad-espacial-mx/index.html#carpetas-de-investigación-de-la-ciudad-de-méxico",
    "href": "posts/intensidad-espacial-mx/index.html#carpetas-de-investigación-de-la-ciudad-de-méxico",
    "title": "Tutorial: Crear mapas de calor (intensidad) para México",
    "section": "Carpetas de investigación de la Ciudad de México",
    "text": "Carpetas de investigación de la Ciudad de México\nPara el propósito de este análisis utilizaremos las carpetas de investigación de la Fiscalía de la Ciudad de México la cual contiene información “de las víctimas de los delitos en las carpetas de investigación de la Fiscalía General de Justicia (FGJ)”.\n\n# Link de datos de la fiscalía de CDMX\nurl <- paste0(\"https://datos.cdmx.gob.mx/dataset/7593b324-6010-44f7-\",\n              \"8132-cb8b2276c842/resource/68304227-862f-4b86-8382-e35\",\n              \"b317a7c39/download/da_victimas_19-20.csv\")\ndf_fiscalia <- read_csv(url, show_col_types = F)\n\nLa base se ve así:\n\n\n\nidCarpetaAño_inicioMes_inicioFechaInicioDelitoCategoriaSexoEdadTipoPersonaCalidadJuridicacompetenciaAño_hechoMes_hechoFechaHechoHoraHechoHoraInicioalcaldia_hechosmunicipio_hechoscolonia_datosfgj_colonia_registrolatitudlongitudnumericnumericcharacterDatecharactercharactercharacternumericcharactercharactercharacternumericcharacterDatehmshmscharactercharactercharactercharacternumericnumeric8,324,4292,019Enero2019-01-04FRAUDEDELITO DE BAJO IMPACTOMasculino62FISICAOFENDIDOFUERO COMUN2,018Agosto2018-08-2943,200.044,340.0ALVARO OBREGONGUADALUPE INNGUADALUPE INN19.4-99.28,324,4302,019Enero2019-01-04PRODUCCIÓN, IMPRESIÓN, ENAJENACIÓN, DISTRIBUCIÓN, ALTERACIÓN O FALSIFICACIÓN DE TÍTULOS AL PORTADOR, DOCUMENTOS DE CRÉDITO PÚBLICOS O VALES DE CANJEDELITO DE BAJO IMPACTOFemenino38FISICAVICTIMA Y DENUNCIANTEFUERO COMUN2,018Diciembre2018-12-1554,000.044,400.0AZCAPOTZALCOVICTORIA DE LAS DEMOCRACIASVICTORIA DE LAS DEMOCRACIAS19.5-99.28,324,4312,019Enero2019-01-04ROBO A TRANSEUNTE SALIENDO DEL BANCO CON VIOLENCIAROBO A CUENTAHABIENTE SALIENDO DEL CAJERO CON VIOLENCIAMasculino42FISICAVICTIMA Y DENUNCIANTEFUERO COMUN2,018Diciembre2018-12-2255,800.044,580.0COYOACANCOPILCO EL BAJOCOPILCO UNIVERSIDAD ISSSTE19.3-99.28,324,4352,019Enero2019-01-04ROBO DE VEHICULO DE SERVICIO PARTICULAR SIN VIOLENCIAROBO DE VEHÍCULO CON Y SIN VIOLENCIAMasculino35FISICAVICTIMA Y DENUNCIANTEFUERO COMUN2,019Enero2019-01-0421,600.044,820.0IZTACALCOPANTITLAN VAGRÍCOLA PANTITLAN19.4-99.18,324,4382,019Enero2019-01-04ROBO DE MOTOCICLETA SIN VIOLENCIAROBO DE VEHÍCULO CON Y SIN VIOLENCIAMasculinoFISICAVICTIMAFUERO COMUN2,019Enero2019-01-0372,000.045,300.0IZTAPALAPALAS AMERICAS (U HAB)PROGRESISTA19.4-99.18,324,4422,019Enero2019-01-04PRODUCCIÓN, IMPRESIÓN, ENAJENACIÓN, DISTRIBUCIÓN, ALTERACIÓN O FALSIFICACIÓN DE TÍTULOS AL PORTADOR, DOCUMENTOS DE CRÉDITO PÚBLICOS O VALES DE CANJEDELITO DE BAJO IMPACTOFemenino42FISICAOFENDIDOFUERO COMUN2,018Octubre2018-10-1264,800.045,480.0COYOACANLOS REYES (PBLO)PUEBLO DE LOS REYES19.3-99.2n: 6\n\n\nLas columnas importantes para este análisis son latitud y longitud las cuales refieren a las coordenadas de los eventos. Por ahora utilizaremos sólo las carpetas sobre ROBO A PASAJERO.\n\ndf_robos <- df_fiscalia |> \n  filter(str_detect(Delito, \"ROBO A PASAJERO\")) |>\n  distinct(latitud, longitud)\n\nLo cual nos deja una lista de coordenadas:\n\n\n\nlatitudlongitudnumericnumeric19.4-99.119.3-99.019.4-99.219.4-99.119.4-99.119.5-99.1n: 6"
  },
  {
    "objectID": "posts/intensidad-espacial-mx/index.html#mapa-de-la-cdmx",
    "href": "posts/intensidad-espacial-mx/index.html#mapa-de-la-cdmx",
    "title": "Tutorial: Crear mapas de calor (intensidad) para México",
    "section": "Mapa de la CDMX",
    "text": "Mapa de la CDMX\nEl mapa de la Ciudad de México lo obtendremos del Instituto Nacional de Estadística y Geografía. En particular éste puede encontrarse en Temas > Marco Geoestadístico y luego seleccionando el marco más reciente y la entidad elegida (o el Nacional que baja todas las entidades).\n\n\n\n\n\n\nNota\n\n\n\nSi quieres replicar éste ejercicio para un mapa específico de otra región no cubierta por el INEGI una fuente con un amplia librería de mapas es la Universidad de Berkeley\n\n\nUna vez descargado el mapa (o descargándolo con el código a continuación) leemos el archivo con la función read_sf del paquete sf (simple features).\n\n# Descargamos del inegi los datos\nmapa_archivo <- tempfile(fileext = \".zip\")\nmapa_dir     <- tempdir()\nurl_mapa <- paste0(\"https://www.inegi.org.mx/contenidos/productos/prod_serv\",\n                   \"/contenidos/espanol/bvinegi/productos/geografia/marcogeo/\",\n                   \"889463770541/09_ciudaddemexico.zip\")\ndownload.file(url_mapa, mapa_archivo, method = \"curl\") #Quizá curl no funcione en windows\n\n# Leemos el mapa\nunzip(mapa_archivo, exdir = mapa_dir)\n\n# Leemos los datos\ncdmx <- read_sf(file.path(mapa_dir,\"09_ciudaddemexico\",\"conjunto_de_datos\",\"09mun.shp\"))\n\nEl resultado de leer los datos en cdmx es un tibble que contiene los polígonos demarcando los municipios de la CDMX. Podemos ver la gráfica de esto con geom_sf:\n\nggplot(cdmx) +\n  geom_sf(aes(geometry = geometry)) +\n  theme_minimal()\n\n\n\n\nPodemos agregar los puntos dentro de la gráfica (ojo a veces los puntos y las gráficas utilizan diferentes proyecciones que hay que homologar)\n\nggplot(cdmx) +\n  geom_sf(aes(geometry = geometry)) +\n  geom_point(aes(x = longitud, y = latitud), data = df_robos) +\n  theme_minimal()\n\n\n\n\nNotamos que algunos de los puntos caen lejos de nuestra unidad de análisis (CDMX) por lo que restringimos los puntos a caer dentro de uno de los polígonos de geom_sf\n\n# Convertimos la latitud y longitud a un objeto `simple features` también\n# el comando crs especifica el sistema de coordenadas usado (viene en el diccionario)\nsf_robos <- df_robos |>\n  st_as_sf(coords = c(\"longitud\", \"latitud\"), crs = \"WGS84\")\n\n# Homologamos los sistemas de coordenadas (el del INEGI tiene otro crs)\nsf_robos <- sf_robos |> st_transform(st_crs(cdmx))\n\n# Intersecamos los robos que ocurrieron en cdmx:\nsf_robos <- sf_robos |> st_intersection(cdmx)\n\nAhora sí ya coinciden:\n\nggplot() +\n  geom_sf(aes(geometry = geometry), data = cdmx) +\n  geom_sf(aes(geometry = geometry), data = sf_robos) + #Ojo aquí cambia a geom_sf\n  theme_minimal()"
  },
  {
    "objectID": "posts/intensidad-espacial-mx/index.html#densidad",
    "href": "posts/intensidad-espacial-mx/index.html#densidad",
    "title": "Tutorial: Crear mapas de calor (intensidad) para México",
    "section": "Densidad",
    "text": "Densidad\nSe puede ajustar una densidad kernel al proceso puntual. Ésta consiste en dividir el espacio \\(\\mathcal{B} \\subset \\mathbb{R}^2\\) en ventanas (pixeles) del mismo tamaño (\\(V_1, V_2, \\dots, V_n\\)) y calcular la cantidad de observaciones (\\(N(V_1), N(V_2), \\dots, N(V_n)\\)) en cada uno de los sitios. La densidad conlleva además un proceso de suavizamiento adicional (el mismo que las densidades kernel de Wasserman (2006) pero bidimensionales) que se encuentra descrito en Ellis (1991).\nPara construir la densidad hay que conertir el objeto sf en un objeto ppp (poisson point process) de la librería spatstat:\n\n# Convertimos a proceso puntual (spatstat)\nrobos_point_process <- as.ppp(sf_robos$geometry, W = as.owin(cdmx))\n\n# Calculamos la densidad de puntos del proceso\nrobos_densidad <- density(robos_point_process)\n\n# Convertimos de vuelta a objeto sf (a través de pointprocess -> stars -> sf)\n# https://www.andrewheiss.com/blog/2023/07/28/gradient-map-fills-r-sf\ndensidad_sf <- robos_densidad |> \n  st_as_stars() |> \n  st_as_sf() |> \n  st_set_crs(st_crs(cdmx)) #Para poner la proyección específica\n\n#Arreglamos los bordes de los pixeles:\ndensidad_sf <- densidad_sf |> st_intersection(cdmx)\n\nFinalmente graficamos:\n\nggplot() +\n  geom_sf(aes(geometry = geometry, fill = v), color = NA, data = densidad_sf) + #Ojo aquí cambia a geom_sf\n  geom_sf(aes(geometry = geometry), data = cdmx, fill = NA, color = \"white\", linewidth = 0.25) +\n  scale_fill_gradientn(guide = \"none\", colours = wes_palette(\"Zissou1\")) +\n  theme_void() + \n  ggtitle(\"Densidad de robos a pasajeros\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPor cierto que la referencia principal para procesos espaciales es Baddeley, Rubak, y Turner (2015)."
  },
  {
    "objectID": "posts/intensidad-espacial-mx/index.html#intensidades-de-papangelou",
    "href": "posts/intensidad-espacial-mx/index.html#intensidades-de-papangelou",
    "title": "Tutorial: Crear mapas de calor (intensidad) para México",
    "section": "Intensidades (de Papangelou)",
    "text": "Intensidades (de Papangelou)\nLa densidad no es la única medida espacial que puede generar mapas de calor. Otra medida de importancia es la intensidad. Todos los procesos de puntos, \\(\\mathcal{X} = \\left\\{X_1, X_2, \\dots, X_k\\right\\}\\), en el espacio son procesos puntuales. Para nuestros propósitos, la intensidad se define como una función \\(\\lambda(u, \\mathcal{X})\\) que depende del proceso (de los puntos en sí, \\(\\mathcal{X}\\)) y de la posición en el espacio \\(u\\).\n\n\n\n\n\n\nNota\n\n\n\nPodemos pensar la intensidad como el cambio infinitesimal en la probabilidad de ocurrencia de un evento en una bola de tamaño \\(\\delta\\) centrada en \\(u\\) condicional al resto de los puntos:\n\\[\n\\lambda(u, \\mathcal{X}) = \\lim_{\\delta \\downarrow 0}\\frac{\\mathbb{P}(\\text{Un evento ocurra en }B_{\\delta}(u) | \\text{El resto del proceso }\\mathcal{X} \\not\\in B_{\\delta(u)})}{\\delta}\n\\] En particular para los modelos que consideraremos, se supone que la intensidad tiene una forma paramétrica:\n\\[\n\\lambda(u, \\mathcal{X}) = \\exp\\big(\\theta^T B(u) + \\gamma^{T} C(u, \\mathcal{X})\\big)\n\\] donde \\(\\theta\\) y \\(\\gamma\\) son parámetros constantes, \\(B(u)\\) es una función de las coordenadas \\(x\\) y \\(y\\) y $ C(u, )$ es una función que depende tanto de las coordenadas como del resto del proceso. Al término \\(B\\) se le denomina tendencia espacial o trend y \\(C\\) se conocen como interacciones estocásticas o interactions. En los modelos de regresión, la forma específica de \\(B\\) y \\(C\\) se especifica eligiendo alguna familia paramétrica. Por ejemplo en los procesos Poisson no homogéneos: \\[\n\\lambda(u, \\mathcal{X}) = b(u)\n\\]\no en los procesos Strauss:\n\\[\n\\lambda(u, \\mathcal{X}) = \\beta \\gamma^{t_r(u,x)}\n\\] donde \\(t_r(u,x)\\) es el número de puntos a una distancia \\(r\\) de \\(u\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nEn general se interpreta la intensidad como una tasa del número de eventos por unidad espacial.\n\n\nEn R podemos ajustar regresiones a los procesos puntuales mediante ppm para obtener la intensidad. En este caso, por ejemplo, ajusto un polinomio de segundo grado a las coordenadas x y y de mi proceso:\n\nfit <- ppm(robos_point_process, trend = ~ polynom(x,y,2), Poisson())\nintensidad <- predict(fit)\n\nSin embargo se pueden agregar otras covariables espaciales (por ejemplo altura ó fecha), así como cambiar la familia del proceso (ver por ejemplo Strauss() y otras ?pairwise.family).\nUna vez estimada la intensidad se convierte de nuevo en un objeto sf y se grafica:\n\n# Convertimos de vuelta a objeto sf (a través de pointprocess -> stars -> sf)\n# https://www.andrewheiss.com/blog/2023/07/28/gradient-map-fills-r-sf\nintensidad_sf <- intensidad |> \n  st_as_stars() |> \n  st_as_sf() |> \n  st_set_crs(st_crs(cdmx)) #Para poner la proyección específica\n\n#Arreglamos los bordes de los pixeles:\nintensidad_sf <- intensidad_sf |> st_intersection(cdmx)\n\nggplot() +\n  geom_sf(aes(geometry = geometry, fill = v), color = NA, data = intensidad_sf) + #Ojo aquí cambia a geom_sf\n  geom_sf(aes(geometry = geometry), data = cdmx, fill = NA, color = \"white\", linewidth = 0.25) +\n  scale_fill_gradientn(guide = \"none\", colours = wes_palette(\"Zissou1\")) +\n  theme_void() +\n  ggtitle(\"Intensidad de robos a pasajeros\")"
  },
  {
    "objectID": "posts/intensidad-espacial-mx/index.html#despedida",
    "href": "posts/intensidad-espacial-mx/index.html#despedida",
    "title": "Tutorial: Crear mapas de calor (intensidad) para México",
    "section": "Despedida",
    "text": "Despedida\nSi en algún momento deseas un artículo de más profundidad sobre procesos puntuales espaciales ¡házmelo saber!"
  },
  {
    "objectID": "posts/interval-regression/index.html",
    "href": "posts/interval-regression/index.html",
    "title": "Bayesian interval regression with Stan",
    "section": "",
    "text": "Consider the following problem where we have measurements for the response variable \\(Y\\) and interval-valued dependent \\(X\\). As an example problem we can think of \\(X\\) as consumption levels for a nutrient (grams per day) and \\(Y\\) can be an outcome (systolic blood pressure). An example database would be as follows:\n\n\nXYfactornumeric(1,3]123.7(5,10]132.6(1,3]120.1(3,5]125.7[0,1]115.8(1,3]128.2(1,3]122.9(10,Inf]141.5(1,3]117.8[0,1]123.8n: 100\n\n\nTo solve this we’ll first review the classical formulation, then the interval-valued formulation and finally we’ll program it in Stan."
  },
  {
    "objectID": "posts/interval-regression/index.html#the-problem",
    "href": "posts/interval-regression/index.html#the-problem",
    "title": "Bayesian interval regression with Stan",
    "section": "",
    "text": "Consider the following problem where we have measurements for the response variable \\(Y\\) and interval-valued dependent \\(X\\). As an example problem we can think of \\(X\\) as consumption levels for a nutrient (grams per day) and \\(Y\\) can be an outcome (systolic blood pressure). An example database would be as follows:\n\n\nXYfactornumeric(1,3]123.7(5,10]132.6(1,3]120.1(3,5]125.7[0,1]115.8(1,3]128.2(1,3]122.9(10,Inf]141.5(1,3]117.8[0,1]123.8n: 100\n\n\nTo solve this we’ll first review the classical formulation, then the interval-valued formulation and finally we’ll program it in Stan."
  },
  {
    "objectID": "posts/interval-regression/index.html#classical-model",
    "href": "posts/interval-regression/index.html#classical-model",
    "title": "Bayesian interval regression with Stan",
    "section": "Classical model",
    "text": "Classical model\nA classical Bayesian linear regression model (if the complete \\(X\\) was observed) is given by: \\[\\begin{equation}\nY|_{X = x} \\sim \\textrm{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2)\n\\end{equation}\\] with priors for the parameters: \\[\\begin{equation}\n\\beta_0 \\sim \\textrm{Normal}(0,100), \\quad \\beta_1 \\sim \\textrm{Normal}(0,100), \\quad \\sigma \\sim \\textrm{Normal}_+(0,100).\n\\end{equation}\\]\nwritten in terms of the likelihood is re presented as: \\[\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}(\\beta_0, \\beta_1, \\sigma | x, y) & \\propto p(y,x | \\beta_0,\\beta_1,\\sigma)\\cdot p(\\beta_0,\\beta_1, \\sigma) \\\\\n& \\propto p(y|x, \\beta_0,\\beta_1,\\sigma) \\cdot p(\\beta_0,\\beta_1, \\sigma)\n\\end{aligned}\n\\end{equation}\\]\nThis term assumes \\(X\\) is a constant and hence there is no need to specify a prior on \\(X\\) nor to include the term \\(p(x| \\beta_0,\\beta_1,\\sigma)\\) in the likelihood. When \\(X\\) is considered a random variable, the distribution of \\(X\\) (say \\(f_X\\)) is included as a prior and in the likelihood too:\n\\[\\begin{equation}\nX \\sim f_ X\\quad \\text{and} \\quad Y|_{X = x} \\sim \\textrm{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2)\n\\end{equation}\\]\nand identical priors for the parameters.\nThe likelihood now includes a term for \\(X\\): \\[\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}(\\beta_0, \\beta_1, \\sigma | x, y)  & \\propto\np(y,x | \\beta_0, \\beta_1, \\sigma) \\cdot p(\\beta_0, \\beta_1, \\sigma) \\\\\n& \\propto p(y|x, \\beta_0,\\beta_1,\\sigma) \\cdot p(x| \\beta_0,\\beta_1,\\sigma)\\cdot p(\\beta_0,\\beta_1, \\sigma).\n\\end{aligned}\n\\end{equation}\\]\nUsually, \\(X\\) is independent from \\(\\beta_0,\\beta_1\\) and \\(\\sigma\\) hence we can write \\(p(x| \\beta_0,\\beta_1,\\sigma) = p(x)\\):\n\\[\\begin{equation}\\label{eqlike}\n\\mathcal{L}(\\beta_0, \\beta_1, \\sigma | x, y) \\propto p(y|x, \\beta_0,\\beta_1,\\sigma)\\cdot p(x) \\cdot p(\\beta_0,\\beta_1, \\sigma).\n\\end{equation}\\]\n\nProgramming this in Stan\nWe’ll start with this model with a lognormal distribution for \\(X\\) as an example:\n\\[\\begin{equation}\nX \\sim \\textrm{Lognormal}(\\mu, \\tau) \\quad \\text{and} \\quad\nY|_{X = x} \\sim \\textrm{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2)\n\\end{equation}\\]\nand priors: \\[\\begin{equation}\n\\beta_0 \\sim \\textrm{Normal}(0,100), \\quad \\beta_1 \\sim \\textrm{Normal}(0,100), \\quad \\sigma \\sim \\textrm{Normal}_+(0,100) \\quad \\tau \\sim \\textrm{Normal}_+(0,100).\n\\end{equation}\\]\nThe stan code is given by:\ndata {\n  int&lt;lower=1&gt; N;       //Total sample size\n  vector&lt;lower=0&gt;[N] X; //Observed X-values\n  vector[N] Y;          //Observed Y-values\n}\n\nparameters {\n  //Mean parameter of X\n  real&lt;lower=0&gt; mu;\n  \n  //Parameters for regression\n  real beta_0;\n  real beta_1;\n  \n  //Parameters for variance\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=0&gt; tau;\n}\n\ntransformed parameters {\n  vector[N] y_mean = rep_vector(beta_0, N) + beta_1*X;\n}\n\nmodel {\n  beta_0 ~ normal(0, 100);\n  beta_1 ~ normal(0, 100);\n  sigma  ~ normal(0, 100);\n  tau    ~ normal(0, 100);\n  X      ~ lognormal(mu, tau);\n  Y      ~ normal(y_mean, sigma);\n}\nyou can save it into a file called regression_linear.stan and fit the model\n\n#Install cmdstanr from their website\n#https://mc-stan.org/cmdstanr/\nlibrary(cmdstanr)\nlibrary(tidybayes)\nset.seed(274865)\n\n#Create an example dataset\nnsamples &lt;- 100\nbeta_0   &lt;- 120\nbeta_1   &lt;- 1.2\nsigma_y  &lt;- 2\nmu_x     &lt;- 1\ntau_x    &lt;- 1.4\n\nexample_db &lt;- tibble(\n  Xtrue = rlnorm(nsamples, meanlog = mu_x, sdlog = tau_x), \n  Y     = beta_0 + beta_1*Xtrue + rnorm(nsamples, sd = sigma_y)\n)\n\nstan_data &lt;- list(\n  N = nrow(example_db),\n  X = example_db$Xtrue,\n  Y = example_db$Y\n)\n\n#Compile the model\nmodel_1 &lt;- cmdstan_model(\"regression_linear.stan\")\n\n#And fit\nfit_model &lt;- model_1$sample(\n   data = stan_data,\n   seed = 4275,\n   chains = 4,\n   parallel_chains = 4,\n   refresh = 0\n)\n\nThe results are very close to the true values:\n\n#We can then verify that the fitted values are very close to the true values\nfit_model |&gt; \n  summarise_draws() |&gt; \n  filter(variable %in% c(\"beta_0\",\"beta_1\",\"sigma\",\"tau\",\"mu\")) |&gt; \n  as_flextable()\n\nvariablemeanmediansdmadq5q95rhatess_bulkess_tailcharacternumericnumericnumericnumericnumericnumericnumericnumericnumericmu0.70.70.10.10.51.01.03,679.42,064.2beta_0119.9119.90.20.2119.5120.31.03,813.03,135.5beta_11.21.20.00.01.21.21.04,488.72,759.6sigma2.12.10.20.21.92.41.04,979.62,797.9tau1.51.50.10.11.31.71.04,420.02,885.7n: 5"
  },
  {
    "objectID": "posts/interval-regression/index.html#interval-censored-model",
    "href": "posts/interval-regression/index.html#interval-censored-model",
    "title": "Bayesian interval regression with Stan",
    "section": "Interval-censored model",
    "text": "Interval-censored model\nWe now consider the case where \\(X\\) is not observed completely but only the ranges of \\(X\\). Survival analysis theory considers three types of ranges:\n\nRight censored: Correspond to values where we only know the lower bound: \\([a,\\infty]\\).\nLeft censored: Are intervals where only the upper bound is known: \\([-\\infty, b]\\).\nInterval censored: Are ranges of \\(X\\) in the shape \\([a,b]\\) where \\(a,b\\) are real numbers.\n\nFor our example, we’ll consider the following dataset where the unknown X is in the range between X_low and X_up. So, this example is only for interval-censored data1.\n\nexample_problem &lt;- \n  tibble(Xtrue = rlnorm(nsamples, meanlog = mu_x, sdlog = tau_x), \n         Y     = beta_0 + beta_1*Xtrue + rnorm(nsamples, sd = sigma_y), \n         X_low = runif(nsamples, 0, Xtrue),\n         X_up  = Xtrue + rlnorm(nsamples, meanlog = 1, sdlog = 1)\n  ) |&gt; \n  select(X_low, X_up, Y)\n  \nas_flextable(example_problem)\n\nX_lowX_upYnumericnumericnumeric1.12.0119.40.10.8120.61.37.8128.23.48.2124.81.64.5123.00.22.9118.31.02.3121.90.52.8123.36.511.0128.32.06.0122.5n: 100\n\n\nIn this case, the likelihood needs to account for the fact that the conditional probability is now for an interval and the conditional joint distribution is required:\n\\[\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}(\\beta_0, \\beta_1, \\sigma | x \\in [a,b], y) & \\propto p(y,x \\in [a,b] | \\beta_0,\\beta_1,\\sigma) \\cdot p(\\beta_0,\\beta_1, \\sigma) \\\\\n& \\propto \\left[\\int\\limits_{a}^{b} p(y | x, \\beta_0,\\beta_1,\\sigma) p(x) dx\\right] \\cdot p(\\beta_0,\\beta_1, \\sigma)\n\\end{aligned}\n\\end{equation}\\]\nwe can program this in Stan by creating this part of the likelihood function separately. Here I’m first summing the log-likelihoods and then exponentiating for numerical stability of the integration process.\nfunctions {\n  // Lognormal probability density function\n  real log_lognormal_pdf(real x, real mu_lognormal, real sigma_lognormal) {\n    //1/(1 / (x * sigma * sqrt(2 * pi())) * exp(- 0.5*((log(x) - mu)/sigma)^2)\n    \n    //Values for logarithm.\n    real log_x = log(x);\n\n    //This corresponds to log(1 / (x * sigma * sqrt(2 * pi())))\n    //The constant is removed as Bayes only requires proportionality\n    real log_normalization = -(log_x + log(sigma_lognormal));\n    \n    //Exponent of the lognormal\n    real exponent = -0.5*square((log_x - mu_lognormal) / sigma_lognormal);\n    \n    //For numerical stability sum in log-scale\n    return log_normalization + exponent;\n  }\n  \n  // Normal probability density function\n  real log_normal_pdf(real y, real mu, real sigma) {\n    //1/(1 / (sqrt(2 * pi())*sigma) * exp(- 0.5*((x - mu)/sigma)^2)\n    \n    //Normal density without the 2*pi as Bayes requires only proportionality\n    real log_normalization = -log(sigma);\n    real log_exponent      = -0.5 * square((y - mu) / sigma);\n\n    return log_normalization + log_exponent;\n  }\n  \n  real integrand_pdf(real x, real xc, array[] real theta, array[] real x_r, array[] int x_i) {\n    //Linear model\n    real mu  = theta[4] + theta[5]*x;\n    \n    //Return the exponential of the log sum\n    return exp(log_lognormal_pdf(x, theta[2], theta[3]) + log_normal_pdf(x_r[1], mu, theta[1]));\n  }\n}\nWe then construct the likelihood in Stan (file regression_interval.stan) using the integrate_1d routine with a precision of \\(\\epsilon^{2/3}\\) as suggested by the Boost website:\n#include integrand.stan\n\ndata {\n  int&lt;lower=1&gt; N;           //Total sample size\n  vector&lt;lower=0&gt;[N] X_low; //Observed lower bound for X\n  vector&lt;lower=0&gt;[N] X_up;  //Observed upper bound for X\n  vector[N] Y;              //Observed Y-values\n}\n\n\ntransformed data {\n  //Normalize Y to help inference process\n  real mu_Y = mean(Y);\n  real sigma_Y = sd(Y);\n  vector[N] y_centered = (Y - mu_Y)/sigma_Y;\n  \n  //Required for integration\n  array[0] int x_i;\n}\n\nparameters {\n  //Mean parameter of X\n  real&lt;lower=0&gt; mu;\n  \n  //Parameters for regression\n  real beta_0;\n  real beta_1;\n  \n  //Parameters for variance\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=0&gt; tau;\n}\n\nmodel {\n  beta_0 ~ normal(0, 100);\n  beta_1 ~ normal(0, 100);\n  sigma  ~ normal(0, 100);\n  tau    ~ normal(0, 100);\n  \n  //Add the likelihood for intervals\n  for (i in 1:N){\n    target += log(integrate_1d(\n      integrand_pdf, X_low[i], X_up[i], \n        {sigma, mu, tau, beta_0, beta_1}, {y_centered[i]}, x_i,\n        pow(machine_precision(), 0.25))\n    );\n  }\n}\n\ngenerated quantities {\n  real beta_0_real = sigma_Y*beta_0 + mu_Y;\n  real beta_1_real = sigma_Y*beta_1;\n  real sigma_real  = sigma_Y*sigma;\n}\nFinally, the R code to infer the parameters which result good estimations of the true values.\n\n#Install cmdstanr from their website\n#https://mc-stan.org/cmdstanr/\nmodel_2 &lt;- cmdstan_model(\"regression_interval.stan\")\n\nstan_data &lt;- list(\n  N     = nrow(example_problem),\n  X_low = example_problem$X_low,\n  X_up  = example_problem$X_up,\n  Y     = example_problem$Y\n)\n\n#And fit\nfit_model &lt;- model_2$sample(\n  data = stan_data,\n  seed = 4275,\n  chains = 1,\n  parallel_chains = 1,\n  #refresh = 0\n)\n\n#We can then verify that the fitted values are very close to the true values\nfit_model |&gt; \n  summarise_draws() |&gt; \n  filter(variable %in% c(\"beta_0_real\",\"beta_1_real\",\"sigma_real\",\"tau\",\"mu\")) |&gt; \n  as_flextable()\n\n\n\nvariablemeanmediansdmadq5q95rhatess_bulkess_tailcharacternumericnumericnumericnumericnumericnumericnumericnumericnumericmu1.21.20.10.11.01.41.0972.6583.6tau1.11.10.10.10.91.21.01,002.1659.7beta_0_real118.6118.60.30.4118.0119.11.0669.6645.5beta_1_real1.41.40.10.11.31.51.0778.2613.0sigma_real1.91.80.20.21.62.21.01,261.1720.2n: 5\n\n\nWhich, graphically, results in the following regression model:\n\n\nShow code for plot\ndraws_model &lt;- fit_model$draws(format = \"draws_df\")\nfor (k in 0:ceiling(max(example_problem$X_up))){\n  temp &lt;- draws_model |&gt; \n    mutate(X = !!k) |&gt; \n    mutate(Y = beta_0_real + beta_1_real*X) |&gt; \n    summarise(\n      Yval = mean(Y),\n      q025 = quantile(Y, 0.025),\n      q975 = quantile(Y, 0.975),\n    ) |&gt; \n    mutate(\n      X = !!k\n    )\n  \n  if (k == 0){\n    results_model &lt;- temp\n  } else {\n    results_model &lt;- results_model |&gt; bind_rows(temp)\n  }\n  \n}\n\nplt &lt;- ggplot(example_problem) +\n  geom_segment(aes(x = X_low, xend = X_up, y = Y, yend = Y, \n                   color = \"Observed\", fill = \"Observed\")) +\n  geom_ribbon(aes(x = X, ymin = q025, ymax = q975, \n                  fill = \"Model\"), data = results_model, alpha = 0.25) + \n  geom_line(aes(x = X, y = Yval, color = \"Model\"), data = results_model, linewidth = 1,\n            linetype = \"dashed\") + \n  theme_minimal() +\n  labs(\n    x = \"X\",\n    y = \"Y\",\n    title = \"Interval-censored regression\"\n  ) +\n  scale_color_manual(\"Data\", values = c(\"Model\" = \"black\", \"Observed\" = \"orange\")) +\n  scale_fill_manual(\"Data\", values = c(\"Model\" = \"deepskyblue4\", \"Observed\" = \"orange\")) +\n  theme(legend.position = \"bottom\")\nggsave(\"Interval.png\", width = 6, height = 4, dpi = 500, plot = plt)"
  },
  {
    "objectID": "posts/interval-regression/index.html#discusion",
    "href": "posts/interval-regression/index.html#discusion",
    "title": "Bayesian interval regression with Stan",
    "section": "Discusion",
    "text": "Discusion\nThis is, of course, not the only way to do this model. A different approach would be to assume that the true \\(X\\) restricted to the interval \\([a,b]\\) has a certain distribution (for example, a Uniform). I find this approach useful (for convergence) but not so intuitive for the priors. Let me know if you’d like a tutorial on this different approach."
  },
  {
    "objectID": "posts/interval-regression/index.html#footnotes",
    "href": "posts/interval-regression/index.html#footnotes",
    "title": "Bayesian interval regression with Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExtensions to right/left can be done including more ifs and using Stan’s special values of positive_infinity() and negative_infinity().↩︎"
  }
]