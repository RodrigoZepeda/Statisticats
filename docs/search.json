[
  {
    "objectID": "posts/regresiones-1/index.html",
    "href": "posts/regresiones-1/index.html",
    "title": "Regresiones (parte 1)",
    "section": "",
    "text": "Adolphe Quetelet fue quien populariz√≥ el uso del m√©todo de m√≠nimos cuadrados en las ciencias sociales. Fuente: Miscellaneous Items in High Demand, PPOC, Library of Congress, Public domain, via Wikimedia Commons."
  },
  {
    "objectID": "posts/regresiones-1/index.html#paquetes-y-datos-a-utilizar-en-r",
    "href": "posts/regresiones-1/index.html#paquetes-y-datos-a-utilizar-en-r",
    "title": "Regresiones (parte 1)",
    "section": "Paquetes y datos a utilizar en R",
    "text": "Paquetes y datos a utilizar en R\nA lo largo de esta secci√≥n usaremos los siguientes paquetes:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)  #autoplot para diagn√≥sticos\nlibrary(rstanarm)   #para regresi√≥n bayesiana\n\n#Siempre que uses tidymodels\ntidymodels_prefer()\n\nEn general usaremos la filosof√≠a tidymodels para combinar los resultados con el tidyverse. Si quieres saber m√°s de tidymodels te recomiendo checar su libro o su p√°gina web.\n\nEmbarazo adolescente y pobreza\nPara los datos usaremos la informaci√≥n de Utts y Heckard para determinar si hay una relaci√≥n entre embarazo adolescente y pobreza.\n\n#Lectura desde sitio web\nurl <- \"https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt\"\nemb_pob <- read_delim(url)\n\nLas variables Brth15to17 y Brth18to19 son las tasas brutas de natalidad por cada 1000 mujeres (en el a√±o 2002) en adolescentes de 15 a 17 a√±os y de 18 a 19 respectivamente. La variable PovPct representa la proporci√≥n (%) de la poblaci√≥n que vive bajo la l√≠nea de pobreza en cada una de las entidades de EEUU (Location). Las variables ViolCrime y TeenBrth no se explican por lo que no las usaremos."
  },
  {
    "objectID": "posts/regresiones-1/index.html#planteamiento-cl√°sico",
    "href": "posts/regresiones-1/index.html#planteamiento-cl√°sico",
    "title": "Regresiones (parte 1)",
    "section": "Planteamiento cl√°sico",
    "text": "Planteamiento cl√°sico\nSi la relaci√≥n fuera perfecta todos los casos caer√≠an exactamente en una l√≠nea recta como sigue:\n\n\n\n\n\ndonde es necesario especificar dos par√°metros: el intercepto (el valor que toma cuando la \\(x\\) en este caso PovPct vale cero) y la pendiente (el valor que relaciona por cada unidad de aumento en PovPct cu√°nto aumenta Brth15to17).\nLa ecuaci√≥n de la l√≠nea est√° dada por:\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\ndonde \\(\\beta_0\\) es el intercepto y \\(\\beta_1\\) la pendiente. Usando la terminolog√≠a de arriba:\n\\[\n\\text{Brth15to17} = \\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}\n\\] en particular en ese ejemplo:\n\\[\n\\text{Brth15to17} = 5 + 3\\cdot \\text{PovPct}\n\\]\nLa idea es que el intercepto (\\(\\beta_0\\) √≥ \\(5\\)) te indica d√≥nde comienza tu l√≠nea cuando no tienes \\(x\\)‚Äôs (es decir cuando \\(\\text{PovPct} = 0\\)). El intercepto controla la altura de la l√≠nea como puedes ver en la siguiente gr√°fica donde puse varios interceptos distintos:\n\n\n\n\n\npor otro lado la idea de la pendiente (\\(\\beta_1\\) √≥ \\(3\\)) es retratar c√≥mo cambia la \\(y\\) (en este caso \\(\\text{Brth15to17}\\)) por cada unidad que cambia la \\(x\\) (en este caso \\(\\text{PovPct}\\)). El valor de \\(3\\) por ejemplo indica que por cada aumento en 1 en \\(\\text{PovPct}\\) la variable \\(\\text{Brth15to17}\\) aumenta en \\(3\\). Este cambio es proporcional; es decir si ahora \\(\\text{PovPct}\\) aumenta 4 (por decir algo) \\(\\text{Brth15to17}\\) aumenta \\(3\\times 4 = 12\\) unidades. La siguiente gr√°fica muestra varias l√≠neas todas comenzando en el mismo intercepto de \\(5\\):\n\n\n\n\n\nComo ya vimos en la primer figura el mundo no es tan perfecto que todo sea una l√≠nea recta. Hay un poco de aleatoriedad involucrada (sea por variables no medidas, por errores de medici√≥n o porque el mundo no sea determinista). Por lo cual se plantea que en lugar de que la \\(y\\) sea exactamente \\(\\beta_0 + \\beta_1 x\\) planteamos que la \\(y\\) proviene de una variable aleatoria normal donde la media de esa normal es \\(\\beta_0 + \\beta_1 x\\); es decir:\n\\[\ny \\sim \\textrm{Normal}( \\beta_0 + \\beta_1 x, \\sigma^2)\n\\]\no dicho de otra manera:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\sigma^2)\n\\]\ndonde la \\(\\sigma^2\\) es la varianza de dicha normal. Puesto gr√°ficamente lo que esto quiere decir es que si, por ejemplo, el intercepto es \\(5\\) y la pendiente \\(3\\) entonces cada medici√≥n de \\(y\\) viene de una normal ligeramente distinta:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(5 + 3\\times \\text{PovPct}, \\sigma^2)\n\\]\n\n\n\n\n\nDicho de otra forma, suponemos que en un mundo perfecto los valores de \\(y\\) (Brth15to17) estar√≠an completamente determinados por los de \\(x\\) (PovPct) mediante la ecuaci√≥n de la recta. Pero como el mundo no es perfecto entonces la \\(y\\) proviene de una normal con promedio dado por la recta. Los puntos (como puedes ver an la siguiente gr√°fica) se centran m√°s en torno a los promedios de las normales sin embargo est√°n colocados aleatoriamente pues corresponden a distintas realizaciones de \\(y\\).\n\n\n\n\n\nPor ejemplo si el porcentaje de pobreza (PovPct) es \\(10\\) entonces la normal de la que provienen estos datos es:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\underbrace{5 + 3\\times 10}_{35}, \\sigma^2)\n\\]\nmientras que si el porcentaje en pobreza es \\(20\\) entonces la normal es:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\underbrace{5 + 3\\times 20}_{65}, \\sigma^2)\n\\]\nPor supuesto que no hay nada de especial con el modelo normal y alguien podr√≠a elegir otra distribuci√≥n (por ejemplo una Gamma) y establecer que:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Gamma}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\beta)\n\\]\nEstos modelos son algunos de los lineales generalizados y los discutiremos m√°s adelante. Por ahora nos quedaremos con la idea del modelo dado por:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\sigma^2)\n\\]\n\nNota quiz√° conoces la regresi√≥n lineal bajo la idea cl√°sica de que \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\] donde \\(\\epsilon\\sim\\text{Normal}(0,\\sigma^2)\\) son los errores normales. Esta definici√≥n es equivalente a la que damos aqu√≠ pues por propiedades aditivas de la normal \\(\\epsilon + \\beta_0 + \\beta_1 x\\) se sigue distribuyendo normal pero con la media ahora dada por lo agregado (\\(\\beta_0 + \\beta_1 x\\)). Las ventajas de esta notaci√≥n es que un modelo para regresi√≥n Poisson es simplemente: \\[\ny \\sim \\textrm{Poisson}\\big(\\exp(\\beta_0 + \\beta_1 x)\\big)\n\\] y un modelo para regresi√≥n log√≠stica es: \\[\ny \\sim \\textrm{Bernoulli}\\big(\\textrm{logit}(\\beta_0 + \\beta_1 x)\\big)\n\\]"
  },
  {
    "objectID": "posts/regresiones-1/index.html#planteamiento-en-r",
    "href": "posts/regresiones-1/index.html#planteamiento-en-r",
    "title": "Regresiones (parte 1)",
    "section": "Planteamiento en R",
    "text": "Planteamiento en R\nLo que nos toca ahora es programar nuestro modelo para ello seguiremos la filosof√≠a de tidymodels que pretende unificar bajo la misma notaci√≥n todos los modelos. La notaci√≥n b√°sica es como sigue:\n\nmodelo %>%\n  set_engine(\"tipo de ajuste\") %>%\n  fit(\"formula a ajustar\", data = tus_datos)\n\nA partir del ajuste se pueden predecir cosas con predict:\n\nmodelo %>%\n  set_engine(\"tipo de ajuste\") %>%\n  fit(\"formula a ajustar\", data = tus_datos) %>%\n  predict()\n\no extraer nuevos datos con extract_fit_engine y tidy:\n\nmodelo %>%\n  set_engine(\"tipo de ajuste\") %>%\n  fit(\"formula a ajustar\", data = tus_datos) %>%\n  extract_fit_engine() %>%\n  tidy()\n\nComencemos con nuestro primer modelo: una regresi√≥n lineal cl√°sica dada por:\n\\[\n\\text{Brth15to17}  \\sim \\textrm{Normal}(\\text{Intercepto} + \\text{Pendiente}\\times \\text{PovPct}, \\sigma^2)\n\\]\nEn R el engine que necesitamos es ‚Äúlm‚Äù que es el cl√°sico:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado <- linear_reg() %>% \n  set_engine(\"lm\") %>%\n  fit(Brth15to17 ~ PovPct, data = emb_pob) #Notaci√≥n y ~ x\n\nNota que a diferencia de Stata, R no arroja demasiados resultados. Podemos usar extract_fit_engine combinado con summary para obtenerlos:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado %>%\n  extract_fit_engine() %>%\n  summary()\n\n\nCall:\nstats::lm(formula = Brth15to17 ~ PovPct, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2275  -3.6554  -0.0407   2.4972  10.5152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.2673     2.5297   1.687    0.098 .  \nPovPct        1.3733     0.1835   7.483 1.19e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.551 on 49 degrees of freedom\nMultiple R-squared:  0.5333,    Adjusted R-squared:  0.5238 \nF-statistic:    56 on 1 and 49 DF,  p-value: 1.188e-09\n\n\no bien con tidy si deseamos nos devuelva una tabla de resultados:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado %>%\n  extract_fit_engine() %>%\n  tidy() \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)     4.27     2.53       1.69 0.0980       \n2 PovPct          1.37     0.184      7.48 0.00000000119\n\n\nSeg√∫n el tipo de regresi√≥n que estemos haciendo es el tipo de tabla que regresa tidy (ver ?tidy). En particular, por ejemplo, podemos modificar para que devuelva intervalos de confianza al 90%:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_ajustado %>%\n  tidy(conf.int = T, conf.level = 0.90)\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic       p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>    <dbl>     <dbl>\n1 (Intercept)     4.27     2.53       1.69 0.0980          0.0260      8.51\n2 PovPct          1.37     0.184      7.48 0.00000000119   1.07        1.68\n\n\nEn este caso, el modelo estima que el intercepto (\\(\\beta_0\\)) es \\(4.27\\) y la pendiente (\\(\\beta_1\\)) es \\(1.37\\). Como son estimadores del verdadero valor se denotan con gorrito: \\(\\hat\\beta_0 = 4.27\\) y \\(\\hat\\beta_1 = 1.37\\).\nPodemos utilizar la funci√≥n de predict para que el modelo nos muestre c√≥mo cree que son los verdaderos valores en relaci√≥n a los ajustados:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\npredichos <- modelo_ajustado %>%\n  predict(new_data = emb_pob)\n\nintervalo_predichos <- modelo_ajustado %>%\n  predict(new_data = emb_pob, type = \"pred_int\", level = 0.95)\n\n#Juntamos los predichos con los observados\nobs_y_modelo <- emb_pob %>% \n  cbind(predichos) %>%\n  cbind(intervalo_predichos)\n\n#Graficamos\nggplot(obs_y_modelo) +\n  geom_ribbon(aes(x = PovPct, ymin = .pred_lower, ymax = .pred_upper), \n              fill = \"#003f5c\", size = 1, alpha = 0.5) +\n  geom_point(aes(x = PovPct, y = Brth15to17), color = \"#bc5090\", size = 3) +\n  geom_line(aes(x = PovPct, y = .pred), \n            color = \"#003f5c\", size = 1) +\n  labs(\n    x = \"Porcentaje en pobreza\",\n    y = \"Tasa bruta de natalidad (por cada 1,000 mujeres adolescentes)\",\n    title = \"Relaci√≥n entre tasa bruta de natalidad en adolescentes\\nde 15 a 19 a√±os y porcentaje en pobreza\"\n  ) +\n  theme_bw()\n\n\n\n\nNada m√°s a ojo no parece que el modelo sea el mejor pues puedes ver que no explica bien la variabilidad (los observados var√≠an mucho respecto al intervalo). La \\(R^2\\), una m√©trica que explica cu√°nto de la varianza captura el modelo tampoco es muy buena:\n\nresumen_ajuste <- modelo_ajustado %>%\n  extract_fit_engine() %>%\n  summary()\n\n#R^2 cl√°sica\nresumen_ajuste$r.squared\n\n[1] 0.533328\n\n#R^2 ajustada\nresumen_ajuste$adj.r.squared\n\n[1] 0.523804\n\n\nPodemos checar las diferentes gr√°ficas de diagn√≥stico:\n\nlibrary(ggfortify)\nautoplot(modelo_ajustado, which = 1:5)\n\n\n\n\nVeamos qu√© significa cada una de ellas y juguemos un poco con R para irlas modificando.\n\nResiduales contra ajustados\nLos residuales son la diferencia entre el modelo (\\(\\hat{y}\\)) y lo real \\(y\\). En el caso que est√°bamos trabajando tenemos que con nuestro modelo podemos predecir los valores de Brth15to17 a partir del porcentaje en pobreza PovPct. A los valores predichos por el modelo de Brth15to17 les ponemos un gorro encima y los llamamos: \\(\\widehat{\\text{Brth15to17}}\\). Estos est√°n dados por la siguiente funci√≥n:\n\\[\n\\widehat{\\text{Brth15to17}} = 4.26 + 1.37 \\cdot \\text{PovPct}\n\\]\npor ejemplo para el porcentaje en pobreza de \\(20.1\\) obtendr√≠amos:\n\\[\n\\widehat{\\text{Brth15to17}} = 4.26 + 1.37 \\cdot \\text{PovPct} = 31.797\n\\]\npor otro lado el verdadero valor de cuando el PovPct es \\(20.1\\) (estado de Alabama) es \\(\\text{Brth15to17} = 31.5\\). La diferencia entre el verdadero valor (\\(\\text{Brth15to17} = 31.5\\)) y el predicho por el modelo (\\(\\widehat{\\text{Brth15to17}} = 31.797\\)) se conoce como el residual. La idea es que en un modelo bueno no debe haber patrones en los residuales (todos deben de flotar en torno al cero pero no mostrar un patr√≥n).\nVeamoslo en nuestra base:\n\n\nC√≥digo\nobs_y_modelo <- obs_y_modelo %>%\n  mutate(residuales = Brth15to17 - .pred)\n\nggplot(obs_y_modelo) +\n  geom_point(aes(x = .pred, y = residuales), color = \"#ff6361\") +\n  labs(\n    x = \"Valores ajustados (predichos)\",\n    y = \"Residuales\",\n    title = \"Residuales vs ajustados\"\n  ) +\n  theme_bw() +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\")\n\n\n\n\n\nEn esta gr√°fica el modelo predice mejor rumbo al final que en medio y esto parece estar corroborado por la gr√°fica del modelo (previa). Nada m√°s para darnos una idea veamos una gr√°fica de malos residuales y una de buenos\n\n\n\n\n\n\n\nEscala locaci√≥n\nRepresenta la escala locaci√≥n contra los residuales estandarizados. La idea de la gr√°fica es ver que la varianza \\(\\sigma^2\\) del modelo no cambie conforme cambia la \\(x\\) (propiedad de homoscedasticidad). Para ello graficamos los residuales estandarizados dados por los residuales mismos dividos entre su desviaci√≥n est√°ndar:\n\\[\nr_{\\text{Std}} = \\frac{\\hat{y} - y}{\\text{sd}(\\hat{y} - y)} = \\frac{\\text{Residuales}}{\\text{sd}\\big(\\text{Residuales}\\big)}\n\\]\nestos residuales estandarizados los podemos calcular en R como sigue:\n\nobs_y_modelo <- obs_y_modelo %>%\n  mutate(residuales_std = residuales/sd(residuales))\n\nSi los graficamos contra los valores ajustados no deber√≠amos de ver ning√∫n patr√≥n:\n\n\nC√≥digo\nggplot(obs_y_modelo) +\n  geom_point(aes(x = .pred, y = residuales_std), color = \"#ff6361\") +\n  labs(\n    x = \"Valores ajustados (predichos)\",\n    y = \"Residuales estandarizados\",\n    title = \"Escala Locaci√≥n\"\n  ) +\n  theme_bw() +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\")\n\n\n\n\n\nPodemos ver c√≥mo se ven estos puntos en el modelo ideal vs en un modelo donde la \\(\\sigma^2\\) depende de la \\(x\\):\n\n\n\n\n\n\n\nNormal cuantil cuantil\nLa segunda gr√°fica corresponde a una gr√°fica cuantil cuantil. Esta la utilizamos para verificar la hip√≥tesis de normalidad. En una gr√°fica cuantil cuantil se grafican los cuantiles de los residuales contra los cuantiles te√≥ricos de la normal. Por ejemplo si la hacemos con s√≥lo 4 puntos se ver√≠a algo as√≠:\n\n\n\n\n\nPodemos armar una gr√°fica cuantil cuantil con ggplot2:\n\nggplot(obs_y_modelo, aes(sample = residuales)) + \n  stat_qq(color = \"#ff6361\") + \n  stat_qq_line(color = \"#58508d\") +\n  theme_bw() +\n  labs(\n    x = \"Cuantiles te√≥ricos de la normal\",\n    y = \"Cuantiles observados de los residuales\",\n    title = \"Gr√°fica qq\"\n  )\n\n\n\n\nLa idea de la gr√°fica cuantil cuantil es que los puntos sigan la l√≠nea lo m√°s posible. Veamos c√≥mo se ve con los datos bien (y los mal)\n\n\n\n\n\n\n\nResiduales contra apalancamiento\nEl apalancamiento representa qu√© tanto cambia el modelo al quitar una sola observaci√≥n. Para poner un ejemplo considera los siguientes datos donde hay un valor at√≠pico y selecciono dos puntos de inter√©s en dos colores:\n\n\n\n\n\nVeamos c√≥mo cambia la regresi√≥n si dejo todos los puntos, si quito el normal y si quito el influyente:\n\n\n\n\n\nNota que el modelo no cambia pr√°cticamente nada cuando hago la regresi√≥n sin el dato que marqu√© como normal pero cambia mucho cuando quito el que marqu√© como influyente. La gr√°fica de residuales contra apalancamiento muestra tambi√©n el valor extra√±o:\n\n\n\n\n\nEl apalancamiento mide la influencia de un dato y en R se puede calcular con hatvalues. Los datos con mayor apalancamiento siempre valen la pena checarlos para verificar que todo opera en orden.\n\n\nC√≥digo\napalancamiento <- modelo_ajustado %>%\n  extract_fit_engine() %>%\n  hatvalues()\n\nobs_y_modelo <- obs_y_modelo %>%\n  cbind(apalancamiento)\n\n#Graficamos residuales contra apalancamiento\nggplot(obs_y_modelo) +\n  geom_point(aes(x = apalancamiento, y = residuales), color = \"#ff6361\") +\n  labs(\n    x = \"Apalancamiento\",\n    y = \"Residuales\",\n    title = \"Residuales vs ajustados\"\n  ) +\n  theme_bw() +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\")\n\n\n\n\n\n\n\nDistancia de Cook\nLa distancia de Cook es un concepto similar al apalancamiento que identifica observaciones influyentes. Aquellos valores con distancia de Cook alta vale la pena revisar. En R podemos usar cooks.distance para calcular la distancia de Cook.\n\n\nC√≥digo\ndistanciaCook <- modelo_ajustado %>%\n  extract_fit_engine() %>%\n  cooks.distance()\n\nobs_y_modelo <- obs_y_modelo %>%\n  cbind(distanciaCook)\n\n#Graficamos residuales contra apalancamiento\nggplot(obs_y_modelo) +\n  geom_col(aes(x = 1:nrow(obs_y_modelo), y = distanciaCook), fill = \"#ff6361\") +\n  labs(\n    x = \"Observaci√≥n (n√∫mero de entrada en la base)\",\n    y = \"Distancia de Cook\",\n    title = \"Distancia de Cook\"\n  ) +\n  theme_bw() \n\n\n\n\n\npodemos ver que en el ejemplo anterior (el de la observaci√≥n influyente) la distancia de Cook es exagerada, tan exagerada que ni se alcanzan a ver los otros:"
  },
  {
    "objectID": "posts/regresiones-1/index.html#ejercicios",
    "href": "posts/regresiones-1/index.html#ejercicios",
    "title": "Regresiones (parte 1)",
    "section": "Ejercicios",
    "text": "Ejercicios\n\nCorre el siguiente c√≥digo para generar una base de datos de nombre datLong que contiene \\(4\\) grupos. Para cada grupo genere una regresi√≥n lineal de la forma:\n\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\nIdentifica cu√°les regresiones s√≠ ajustan bien y cu√°les no mediante los gr√°ficos de diagn√≥stico. Finalmente grafica tus datos \\(x\\) contra \\(y\\) y la regresi√≥n para ver que lo hayas hecho bien. ¬øHay alguna forma de corregir alguna de las que no ajusta bien?\n\ndat <- datasets::anscombe\ndatLong <- data.frame(\n    grupo  = rep(1:4, each = 11),\n    x = unlist(dat[,c(1:4)]),\n    y = unlist(dat[,c(5:8)])\n    )\nrownames(datLong) <- NULL\n\n\nLee la base de datos de \\(n = 345\\) ni√±os entre \\(6\\) y \\(10\\) a√±os de Kahn, Michael (2005). Las variables de inter√©s son \\(y = \\text{FEV}\\) el volumen de expiraci√≥n forzada y \\(x = \\text{edad}\\) en a√±os. Realiza una regresi√≥n lineal. Justifica que no se cumple la homocedasticidad mediante una gr√°fica de escala locaci√≥n.\nPlantee una regresi√≥n lineal usando los datos de esta liga para determinar si el sexo influye en el salario. Ojo en la regresi√≥n es necesario incluir otras covariables."
  },
  {
    "objectID": "posts/regresiones-1/index.html#y-si-lo-hacemos-bayesiano",
    "href": "posts/regresiones-1/index.html#y-si-lo-hacemos-bayesiano",
    "title": "Regresiones (parte 1)",
    "section": "¬øY si lo hacemos bayesiano?",
    "text": "¬øY si lo hacemos bayesiano?\nPara hacer la misma regresi√≥n lineal pero con estad√≠stica bayesiana podemos nada m√°s cambiar el engine:\n\n#Ajusta un modelo lineal \n#Brth15to17 = intercepto + pendiente*PovPct\nmodelo_bayesiano <- linear_reg() %>% \n  set_engine(\"stan\") %>%\n  fit(Brth15to17 ~ PovPct, data = emb_pob) #Notaci√≥n y ~ x\n\nY podemos ver el ajuste:\n\nmodelo_bayesiano %>%\n  extract_fit_engine() %>%\n  summary()\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      Brth15to17 ~ PovPct\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 51\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 4.3    2.6  0.9   4.3   7.7  \nPovPct      1.4    0.2  1.1   1.4   1.6  \nsigma       5.7    0.6  4.9   5.6   6.4  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 22.3    1.1 20.9  22.3  23.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  4252 \nPovPct        0.0  1.0  4286 \nsigma         0.0  1.0  3968 \nmean_PPD      0.0  1.0  4057 \nlog-posterior 0.0  1.0  1705 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\no realizar predicciones:\n\npredichos <- modelo_bayesiano %>%\n  extract_fit_engine() %>%\n  predict(new_data = emb_pob)\n\nic <- modelo_bayesiano %>%\n  extract_fit_engine() %>%\n  predictive_interval(newdata = emb_pob, prob = 0.95) #Para bayesiana\n\n#Juntamos los predichos con los observados\nobs_y_modelo <- emb_pob %>% \n  cbind(predichos) %>%\n  cbind(ic)\n\n#Graficamos\nggplot(obs_y_modelo) +\n  geom_ribbon(aes(x = PovPct, ymin = `2.5%`, ymax = `97.5%`), \n              fill = \"#003f5c\", size = 1, alpha = 0.5) +\n  geom_point(aes(x = PovPct, y = Brth15to17), color = \"#ffa600\", size = 3) +\n  geom_line(aes(x = PovPct, y = predichos), \n            color = \"#003f5c\", size = 1) +\n  labs(\n    x = \"Porcentaje en pobreza\",\n    y = \"Tasa bruta de natalidad (por cada 1,000 mujeres adolescentes)\",\n    title = \"Relaci√≥n entre tasa bruta de natalidad en adolescentes\\nde 15 a 19 a√±os y porcentaje en pobreza\"\n  ) +\n  theme_bw()\n\n\n\n\nPara validaci√≥n del modelo puedes checar esta p√°gina que explica loo (ver paper:\n\nmodelo_bayesiano %>%\n  extract_fit_engine() %>% \n  loo()\n\n\nComputed from 4000 by 51 log-likelihood matrix\n\n         Estimate  SE\nelpd_loo   -161.6 4.2\np_loo         2.4 0.5\nlooic       323.3 8.4\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nasi como su visualizaci√≥n (si el modelo no fuera bueno)\n\nmodelo_bayesiano %>%\n  extract_fit_engine() %>% \n  loo() %>%\n  plot(label_points = TRUE)"
  },
  {
    "objectID": "posts/regresiones-1/index.html#ejercicio",
    "href": "posts/regresiones-1/index.html#ejercicio",
    "title": "Regresiones (parte 1)",
    "section": "Ejercicio",
    "text": "Ejercicio\n\nUtilice las opciones de engine para cambiar el prior_intercept del intercepto a una t de Student y el prior de los coeficientes a una Laplace. ¬øCambia mucho el resultado?"
  },
  {
    "objectID": "posts/Navidad/index.html#sec-spanish-tutorial",
    "href": "posts/Navidad/index.html#sec-spanish-tutorial",
    "title": "Navidad/Christmas @ #rstats",
    "section": "Tutorial en espa√±ol",
    "text": "Tutorial en espa√±ol\nEsta Navidad las Rladies de Quer√©taro hicieron un concurso para realizar un √°rbol de Navidad con R y decid√≠ participar. Esta vez no quer√≠a hacer el t√≠pico arbolito con ggplot2 as√≠ que decid√≠ probar otra tecnolog√≠a: rayrender.\nrayrender es una librer√≠a para crear im√°genes 3D mediante trazado de rayos (raytracing). No es la mejor opci√≥n open source para la tarea pero ¬°hey est√° en R!\nLa forma en la que rayrender funciona es consturyendo una escena y a partir de √©sta agregar objetos a la escena. Podemos empezar con una escena vac√≠a de estudio fotogr√°fico:\n\nlibrary(rayrender)\nscene_test <- generate_studio(depth = 0.2, \n                         material = diffuse(checkercolor = \"red\"))\n\nPara poder ver previsualizar la escena se utiliza render_scene:\n\nrender_scene(scene_test, \n             samples = 100, #Muestras (+ muestras menos ruido)\n             preview = T,   #Si quieres previsualizar la escena antes de calcular\n             parallel = T   #Si realizar el c√≥mputo en paralelo\n             )\n\n\n\n\n\n\n\n\n\n\nSobre la escena podemos agregar objetos con add_object. En particular podemos agregar un cilindro para el tronco especificando sus coordenadas as√≠ como el material:\n\nscene_test <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nHay diferentes materiales, por ejemplo metal, cabello hair, luz light, cristal dielectric, etc (la lista completa en el apartado materials) por lo que si quisi√©ramos hacer nuestro √°rbol de cristal bastar√≠a cambiar el material:\n\nscene_metal <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = metal(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nPodemos cambiar la perspectiva de la c√°mara ajustando manualmente y dando p para obtener las coordenadas y luego imputarlas en el render:\n\nrender_scene(scene_metal, \n             samples = 100, \n             preview = T, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), #D√≥nde est√° la c√°mara\n             lookat   = c(1, 5, 0), #D√≥nde est√° viendo la c√°mara\n             aperture = 0.5, #Apertura \n             fov = 17,\n             focal_distance = 77.66, #Distancia focal\n             iso = 400, #Sensibilidad \"del rollo fotogr√°fico\" a la luz \n             clamp_value = 10\n             )\n\n\n\n\n\n\n\n\n\n\nSobre nuestro √°rbol podemos agregar conos verdes. Agregamos varios conos en un loop para darle mayor figura. Lo pondremos sobre un nuevo fondo (blanco) y con el tronco que hicimos previamente usando diffuse:\n\nscene <- generate_studio(depth = 0.2) |>\n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  ) \n\n#Conos\nfor (i in seq(1, 10, length.out = 20)){\n  scene <- scene |> \n    add_object(\n      cone(\n        start  = c(0, 6 - i/2, 0),\n        end    = c(0, 6 - (i/2 - 1), 0),\n        radius = i/3,\n        material = diffuse(color = \"darkgreen\")\n      )\n    )\n}\n\n\n\n\n\n\n\n\n\n\nFinalmente agregamos esferas luminosas de dos colores distintos: amarillas y rojas sobre las superficies de los conos aleatoriamente:\n\nset.seed(27522)\n#Esferas luminosas\nfor (i in seq(1, 10, length.out = 20)){\n  #Esferas rojas\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    #Altura de la esfera\n        x = cos(k)*i/3, #Coordenadas polares para superficie de cono\n        z = sin(k)*i/3,\n        material = light(color = \"red\", intensity = 5)\n      )\n    )\n  }\n  #Esferas amarillas\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    #Altura de la esfera\n        x = cos(k)*i/3, #Coordenadas polares para superficie de cono\n        z = sin(k)*i/3,\n        material = light(color = \"#f8d568\", intensity = 5)\n      )\n    )\n  }\n}\n\n\n\n\n\n\n\n\n\n\nLa estrella se agrega hasta arriba con un pol√≠gono extruded_polygon que permite dise√±ar figuras.\n\n#Adaptado de la estrella de https://www.rayrender.net/index.html\nangulos  <- seq(0, 2*pi, length.out = 11)\nx        <- rev(c(rep(c(1,0.5), 5), 1)) * cos(angulos)\nz        <- rev(c(rep(c(1,0.5), 5), 1)) * sin(angulos)\npoligono <- data.frame(x = x, z = z)\nestrella <- rbind(poligono, 0.8*poligono)\n\n#Agregamos la estrella luminosa a la escena\nscene <- scene |>\n  add_object(\n    extruded_polygon(\n      estrella,\n      top = -0.5,\n      bottom = -1,\n      y = 7,    #Altura\n      z = 0.75, #Centrar\n      angle = c(90, 0, 90),\n      material = light(color = \"white\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nUna vez que est√° listo nuestro √°rbol nos preparamos para renderizarlo con suficientes muestras para eliminar todo el ruido:\n\n\n\n\n\n\nWarning\n\n\n\n√âste proceso tarda varias horas\n\n\n\npng(\"arbolito.png\")\nrender_scene(scene, \n             width = 500,    #Ancho en pixeles\n             height = 500,   #Alto en pixeles\n             samples = 1000, #Suficientes muestras!\n             preview = F, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), \n             lookat   = c(1, 5, 0), \n             aperture = 0.5, \n             fov = 17,\n             focal_distance = 77.66,\n             iso = 400, \n             clamp_value = 10\n             )\ndev.off()\n\n\n\n\n\n\n\n\n\n\nFinalmente al arbolito le agregamos texto de Feliz Navidad\n\nlibrary(png)\nlibrary(showtext)\nlibrary(cowplot)\nlibrary(ggplot2)\n\n#Descarga de la fuente Passions Conflict\nfont_add_google(\"Passions Conflict\", \"pconflict\")\nshowtext_auto()\n\n#Leemos la imagen\narbol <- readPNG(\"arbolito.png\")\n\ndrawplot <- ggdraw() +\n  annotation_raster(arbol, xmin = 0, ymin = 0, xmax = 1, ymax = 1) +\n  geom_text(aes(x = 0.5, y = 0.1, label = \"Feliz Navidad\"), color = \"white\",\n            family = \"pconflict\", size = 15) +\n  geom_text(aes(x = 0.5, y = 0.95,\n                label = \"@RodZepeda | rodrigozepeda.github.io/Statisticats/posts/Navidad\"),\n            color = \"gray75\",\n            size = 3) \nggsave(\"arbolito_navidad_2022_es.png\", drawplot, dpi = 100, width = 500, height = 500, units = \"px\")"
  },
  {
    "objectID": "posts/Navidad/index.html#sec-english-tutorial",
    "href": "posts/Navidad/index.html#sec-english-tutorial",
    "title": "Navidad/Christmas @ #rstats",
    "section": "Tutorial in English",
    "text": "Tutorial in English\n\n\n\n\n\n\nTutorial en espa√±ol\n\n\n\nPara el tutorial en espa√±ol ve a Section¬†1.\n\n\nThis Christmas Rladies Quer√©taro created a contest. One was to build a Christmas tree in R. I decided to participate. This time I didn‚Äôt want to do the typical ggplot2 tree so I decided to test another technology: rayrender.\nrayrender is a library to create 3D images using raytracing. This is not the best open source option for this task, but hey, it‚Äôs in R!\nThe way rayrender works is by building a scene and adding objects to it. We can start with an empty study scene:\n\nlibrary(rayrender)\nscene_test <- generate_studio(depth = 0.2, \n                         material = diffuse(checkercolor = \"red\"))\n\nTo preview the scene use render_scene:\n\nrender_scene(scene_test, \n             samples = 100, #More samples less noise\n             preview = T,   #To preview the scene (before calculating)\n             parallel = T   #Compute in parallel for extra speed\n             )\n\n\n\n\n\n\n\n\n\n\nWe can add different objects onto the scene with add_object. In particular, we can add a cylinder for the tree trunk as well as its coordinates:\n\nscene_test <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nThere are different materials (e.g.¬†metal, hair, light, (crystal) dielectric, etc see materials). If we wish to make our tree metal we can just change the material:\n\nscene_metal <- scene_test |> \n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = metal(color = \"#725c42\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nWe can shift the camera‚Äôs perspective by manually adjusting and obtain the coordinates with p. These coordinates can be inputed into the render:\n\nrender_scene(scene_metal, \n             samples = 100, \n             preview = T, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), #Where camera is\n             lookat   = c(1, 5, 0), #What is camera watching\n             aperture = 0.5, \n             fov = 17,\n             focal_distance = 77.66, \n             iso = 400, #Sensitivity to light\n             clamp_value = 10\n             )\n\n\n\n\n\n\n\n\n\n\nWe can add green cones to our tree. We do that with a for loop. We‚Äôll put that tree over a white background and with the previous trunk we had using diffuse:\n\nscene <- generate_studio(depth = 0.2) |>\n  add_object(\n    cylinder(\n      x = 0,\n      y = 0,\n      z = 0,\n      radius = 0.25,\n      length = 10,\n      material = diffuse(color = \"#725c42\")\n    )\n  ) \n\n#Cones\nfor (i in seq(1, 10, length.out = 20)){\n  scene <- scene |> \n    add_object(\n      cone(\n        start  = c(0, 6 - i/2, 0),\n        end    = c(0, 6 - (i/2 - 1), 0),\n        radius = i/3,\n        material = diffuse(color = \"darkgreen\")\n      )\n    )\n}\n\n\n\n\n\n\n\n\n\n\nFinally, we add light spheres of two colors: yellow and red. These go randomly over the cone‚Äôs surface.\n\nset.seed(27522)\nfor (i in seq(1, 10, length.out = 20)){\n  #Red spheres\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    #Sphere height\n        x = cos(k)*i/3, #Polar coordinates for the cone's surface\n        z = sin(k)*i/3,\n        material = light(color = \"red\", intensity = 5)\n      )\n    )\n  }\n  #yellow spheres\n  for (k in runif(2*i, 0, 2*pi)){\n  scene <- scene |> \n    add_object(\n      sphere(\n        radius = 0.1,\n        y = 6 - i/2,    \n        x = cos(k)*i/3, \n        z = sin(k)*i/3,\n        material = light(color = \"#f8d568\", intensity = 5)\n      )\n    )\n  }\n}\n\n\n\n\n\n\n\n\n\n\nWe add an extruded_polygon for the star:\n\n#Adapted from the star at https://www.rayrender.net/index.html\nangulos  <- seq(0, 2*pi, length.out = 11)\nx        <- rev(c(rep(c(1,0.5), 5), 1)) * cos(angulos)\nz        <- rev(c(rep(c(1,0.5), 5), 1)) * sin(angulos)\npoligono <- data.frame(x = x, z = z)\nestrella <- rbind(poligono, 0.8*poligono)\n\n#Add the luminous star\nscene <- scene |>\n  add_object(\n    extruded_polygon(\n      estrella,\n      top = -0.5,\n      bottom = -1,\n      y = 7,    #Altura\n      z = 0.75, #Centrar\n      angle = c(90, 0, 90),\n      material = light(color = \"white\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nOnce our tree is ready we render it with enough samples to eliminate the noise:\n\n\n\n\n\n\nWarning\n\n\n\nThis process takes several hours\n\n\n\npng(\"arbolito.png\")\nrender_scene(scene, \n             width = 500,    #Pixel width\n             height = 500,   #Pixel height\n             samples = 1000, #Enough samples\n             preview = F, \n             parallel = T,\n             lookfrom = c(50.10, 7.25, 60.13), \n             lookat   = c(1, 5, 0), \n             aperture = 0.5, \n             fov = 17,\n             focal_distance = 77.66,\n             iso = 400, \n             clamp_value = 10\n             )\ndev.off()\n\n\n\n\n\n\n\n\n\n\nFinally we add Merry Christmas text to the tree\n\nlibrary(png)\nlibrary(showtext)\nlibrary(cowplot)\nlibrary(ggplot2)\n\n#Download Passions Conflict from google fonts\nfont_add_google(\"Passions Conflict\", \"pconflict\")\nshowtext_auto()\n\n#Leemos la imagen\narbol <- readPNG(\"arbolito.png\")\n\ndrawplot <- ggdraw() +\n  annotation_raster(arbol, xmin = 0, ymin = 0, xmax = 1, ymax = 1) +\n  geom_text(aes(x = 0.5, y = 0.1, label = \"Merry Christmas\"), color = \"white\",\n            family = \"pconflict\", size = 15) +\n  geom_text(aes(x = 0.5, y = 0.95,\n                label = \"@RodZepeda | rodrigozepeda.github.io/Statisticats/posts/Navidad\"),\n            color = \"gray75\",\n            size = 3) \nggsave(\"arbolito_navidad_2022_en.png\", drawplot, dpi = 100, width = 500, height = 500, units = \"px\")"
  },
  {
    "objectID": "posts/memoise/index.html",
    "href": "posts/memoise/index.html",
    "title": "FasteR functions with memoise",
    "section": "",
    "text": "Memoisation is a technique for speeding up functions via memorization of previously calculated results. To better explain the idea let‚Äôs consider a recursive formulation of the Fibonacci sequence:\n\\[\nf(n) = f(n- 1) + f(n-2)\n\\]\nwith \\(f(1) = 1 = f(2)\\). 1\nAn implementation of the function is given by:\n\nfibonacci <- function(n){\n  if(n <= 2){\n    return(1)\n  } else {\n    return(\n      fibonacci(n - 1) + fibonacci(n - 2)\n    )\n  }\n}\n\nWe can calculate the time it takes to estimate the number up to 20:\n\nmicrobenchmark::microbenchmark(fibonacci(20))\n\nUnit: milliseconds\n          expr      min       lq    mean   median       uq      max neval\n fibonacci(20) 4.633172 5.810971 6.92568 6.418399 7.803796 17.74069   100\n\n\nLarger values (like fibonacci(100)) start taking a lot of time. And that‚Äôs because fibonacci(100) estimates the same values several times! To illustrate this point, consider fibonacci(5). You can see that fibonacci(3) is estimated twice: once under fibonacci(5) itself and one under fibonacci(4). This is extremely inefficient!\n\n\n\n\ngraph TD\n    f5[fibonacci 5] --> f4[fibonacci 4]\n    f5[fibonacci 5] --> f3[fibonacci 3]\n    f3 --> f13[fibonacci 1]\n    f3 --> f23[fibonacci 2]\n    f4 --> f32[fibonacci 3]\n    f4 --> f2[fibonacci 2]\n    f32 --> f21[fibonacci 2]\n    f32 --> f11[fibonacci 1]\n\n\n\n\n\n\n\n\nWe can calculate how many times the function fibonacci is called when estimating different numbers. In theory it should scale linearly i.e. fibonacci(20) should only calculate fibonacci(1), fibonacci(2), etc up to fibonacci(19) once. Hence the function should be called at most 20 times. However this isn‚Äôt the case:\n\ncalls_f   <- 0\nfibonacci_calls <- function(n){\n  calls_f <<- calls_f + 1\n  if(n <= 2){\n    return(1)\n  } else {\n    return(\n      fibonacci_calls(n - 1) + fibonacci_calls(n - 2)\n    )\n  }\n}\ninvisible(fibonacci_calls(20))\n\ncat(paste(\"fibonacci_calls was called\",calls_f,\"times\"))\n\nfibonacci_calls was called 13529 times\n\n\nThe algorithm is extremely inefficient because it doesn‚Äôt remember when estimating fibonacci_calls(8) that it already has estimated fibonacci_calls(7) during its estimation of fibonacci_calls(9). Memoisation is a solution for this forgetfulness."
  },
  {
    "objectID": "posts/memoise/index.html#sec-hard-coded",
    "href": "posts/memoise/index.html#sec-hard-coded",
    "title": "FasteR functions with memoise",
    "section": "Hard coded memoisation",
    "text": "Hard coded memoisation\nAs we have seen, fibonacci(20) calls the fibonacci function 13,529 times. This inefficiency could be saved if the function could memorize that it has already calculated the previous results. That is the utility of the memoisation trick. To save these memorizations (memoise!), let‚Äôs create a global vector where we‚Äôll keep the previous results of the fibonacci function. That is, the j-th entry of our vector will correspond to fibonacci(j).\n\n#Fibonacci cache vector up til fibonacci 1000 \n#fibonacci_cache[j] = fibonacci(j)\nfibonacci_cache <- rep(NA, 1000)\n\n#fibonacci_cache[1] = fibonacci(1) and fibonacci_cache[2] = fibonacci(2) \nfibonacci_cache[1:2] <- c(1, 1)\n\nmemoised_fibonacci <- function(n){\n  #Only calculate the values we haven't previously estimated\n  if (is.na(fibonacci_cache[n])){\n    fibonacci_cache[n] <<- memoised_fibonacci(n - 1) + memoised_fibonacci(n - 2)\n  } \n  return(fibonacci_cache[n])\n}\n\n\n\n\nLet‚Äôs compare the speed of the previous function with this one:\n\nmicrobenchmark::microbenchmark(fibonacci(20), memoised_fibonacci(20))\n\nUnit: nanoseconds\n                   expr     min        lq       mean    median      uq     max\n          fibonacci(20) 3942361 4796700.5 5059247.18 4838703.0 4977222 8489579\n memoised_fibonacci(20)     398     589.5    2730.04    1642.5    3855   28640\n neval\n   100\n   100\n\n\nThis speed-up happens because the new memoised_fibonacci only calculates the value 18 times! In contrast with the previous 13,529. For any custom function you build you can memoise this way ooooor you can let the memoise package do it for you."
  },
  {
    "objectID": "posts/memoise/index.html#the-memoise-package",
    "href": "posts/memoise/index.html#the-memoise-package",
    "title": "FasteR functions with memoise",
    "section": "The memoise package",
    "text": "The memoise package\nThe memoise package does exactly what we did in the previous section by automatically memoising functions (with arguments that aren‚Äôt necessarily numbers). It sets limits to the memory (our fibonacci_cache) as well as the amount of time a function will remember previous results (time limit). To memoise a function you just need to call memoise over it:\n\nlibrary(memoise)\nmfibo <- function(n){\n  if(n <= 2){\n    return(1)\n  } else {\n    return(\n      mfibo(n - 1) + mfibo(n - 2) #It's important to call this with the memoized name\n    )\n  }\n}\nmfibo <- memoise(mfibo) #Memoization line\n\n\n\n\nNote that this memoisation has additional overhead over the memoisation we did in Section¬†2 because memoise works even for non numeric arguments (which would have failed in our vectorized example). However the speed-up over the original is still there:\n\nmicrobenchmark::microbenchmark(fibonacci(20), mfibo(20), memoised_fibonacci(20)) \n\nUnit: nanoseconds\n                   expr     min        lq       mean  median        uq      max\n          fibonacci(20) 5291457 6031574.5 7138627.87 6599172 7929635.0 14128101\n              mfibo(20)   58632   72422.5  151487.39  139772  190614.5   569502\n memoised_fibonacci(20)     606    1206.0    5186.56    3470    7506.5    39376\n neval\n   100\n   100\n   100\n\n\nThe memoise package uses cachem which allows for fine control over where the previous results of the function. You can substitute the #Memoization line in the previous code for this memoise with finer control.\n\n#Set memory to 100MB and time to memorizing only for 15 minutes\ncm    <- cachem::cache_mem(max_size = 100 * 1024^2, max_age = 15 * 60)\nmfibo <- memoise(mfibo, cache = cm)\n\nTo keep previous computations across different R sessions you can cache directly to disk (slower) instead of memory:\n\ncm    <- cachem::cache_disk(max_size = 100 * 1024^2, max_age = 15 * 60)\nmfibo <- memoise(mfibo, cache = cm)"
  },
  {
    "objectID": "posts/memoise/index.html#and-in-other-languages",
    "href": "posts/memoise/index.html#and-in-other-languages",
    "title": "FasteR functions with memoise",
    "section": "And in other languages?",
    "text": "And in other languages?\nYou can also memoise in the most recent versions of Python either vie the built in functools or the memoization project. In Julia you can Memoize.jl. Let me know if you are interested in an entry for any of these."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statisticats üê±",
    "section": "",
    "text": "FasteR functions with memoise\n\n\n\n\n\n\n\nR\n\n\nmemoise\n\n\nAdvanced R / R avanzado\n\n\nprogramming tips / tips de programaci√≥n\n\n\n\n\nIn this entry I discuss how to speed up recursive R functions with the memory-trick of memoise.\n\n\n\n\n\n\nDec 27, 2022\n\n\nRodrigo Zepeda-Tello\n\n\n\n\n\n\n  \n\n\n\n\nNavidad/Christmas @ #rstats\n\n\nIntro to/a rayrender\n\n\n\n\nR\n\n\nrayrender\n\n\nNavidad / Christmas\n\n\ngr√°ficas / plots\n\n\n\n\nEn esta entrada discutimos c√≥mo hacer un arbolito de Navidad con rayrender. In this entry we discuss how to build a Christmas tree with rayrender\n\n\n\n\n\n\nDec 22, 2022\n\n\nRodrigo Zepeda-Tello\n\n\n\n\n\n\n  \n\n\n\n\nRegresiones (parte 1)\n\n\n\n\n\n\n\nregresiones / regression\n\n\nregresi√≥n lineal / linear regression\n\n\nR\n\n\nespa√±ol / Spanish\n\n\ndiagn√≥stico de modelos / model diagnosis\n\n\n\n\nEn esta entrada discutimos los fundamentos de una regresi√≥n lineal (punto de vista probabil√≠stico) as√≠ como su programaci√≥n y diagn√≥stico en R\n\n\n\n\n\n\nNov 6, 2022\n\n\nRodrigo Zepeda-Tello\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About | Acerca de",
    "section": "",
    "text": "Applied mathematician. I spend my time modelling (but with math). Some years ago I became a statistician by mistake: haven‚Äôt been able to leave the club since."
  },
  {
    "objectID": "about.html#espa√±ol",
    "href": "about.html#espa√±ol",
    "title": "About | Acerca de",
    "section": "Espa√±ol:",
    "text": "Espa√±ol:\nMatem√°tico aplicado. Me la vivo modelando (pero con matem√°ticas). Hace algunos a√±os me convert√≠ en un estad√≠stico por error: no he podido salir de este club desde entonces."
  }
]